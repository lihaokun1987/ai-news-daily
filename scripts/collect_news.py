#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ–°é—»èšåˆåŠ©æ‰‹ - æ–°é—»é‡‡é›†ä¸åˆ†æè„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
æ”¹è¿›ï¼š
1. æ·»åŠ Twitter/Xã€Redditç­‰å¤šå¹³å°æ•°æ®æºï¼ˆå‚è€ƒé¡¹ç›®1ï¼‰
2. ä½¿ç”¨Googleç¿»è¯‘APIè¿›è¡Œæ™ºèƒ½ç¿»è¯‘ï¼Œç¡®ä¿è¾“å‡ºç»Ÿä¸€ä¸ºä¸­æ–‡
3. ä¼˜åŒ–è¯„åˆ†æ’åºé€»è¾‘
4. æ‰©å±•æœç´¢å…³é”®è¯è¦†ç›–èŒƒå›´
5. ã€æ–°å¢ã€‘RSSä¼˜å…ˆæŠ“å–ç­–ç•¥ï¼Œæé«˜ç¨³å®šæ€§ï¼Œé™ä½åçˆ¬é£é™©
"""
import os
import sys
import json
import re
import hashlib
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

# å°è¯•å¯¼å…¥ç¿»è¯‘åº“
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
    print("âœ… Googleç¿»è¯‘APIå·²åŠ è½½")
except ImportError:
    print("è­¦å‘Šï¼šdeep-translatoræœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨è¯å…¸ç¿»è¯‘")
    print("å®‰è£…å‘½ä»¤: pip install deep-translator")
    GoogleTranslator = None
    TRANSLATOR_AVAILABLE = False

# å°è¯•å¯¼å…¥MCPå·¥å…·ï¼ˆå‚è€ƒé¡¹ç›®1çš„æ–¹å¼ï¼‰
# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)
sys.path.append(PROJECT_DIR)
try:
    from mcp_matrix import batch_web_search, twitter_search_tweets, extract_content_from_websites
    MCP_AVAILABLE = True
except ImportError:
    print("è­¦å‘Šï¼šMCPå·¥å…·å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆç›´æ¥ç½‘é¡µæŠ“å–ï¼‰")
    batch_web_search = None
    twitter_search_tweets = None
    extract_content_from_websites = None
    MCP_AVAILABLE = False


class AINewsCollector:
    """AIæ–°é—»æ”¶é›†å™¨ï¼ˆå¢å¼ºç‰ˆ - å‚è€ƒé¡¹ç›®1ä¼˜åŒ–ï¼‰"""
    
    def __init__(self):
        self.date = datetime.now().strftime('%Y-%m-%d')
        self.yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        self.news_items = []
        
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œè‡ªåŠ¨è·å–é¡¹ç›®æ ¹ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(script_dir)
        self.output_dir = os.path.join(project_dir, 'output')
        self.logs_dir = os.path.join(project_dir, 'logs')
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.logs_dir, exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(self.logs_dir, f'collect_{datetime.now().strftime("%Y%m%d")}.log')
        
        # HTTPä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'
        })
        
        # æ‰©å±•çš„ç¿»è¯‘å…³é”®è¯æ˜ å°„ï¼ˆä¿ç•™åŸæœ‰ + æ–°å¢æ›´å¤šï¼‰
        self.translate_map = {
            # ä¿æŒåŸæ ·çš„å“ç‰Œå
            'ChatGPT': 'ChatGPT',
            'OpenAI': 'OpenAI',
            'Anthropic': 'Anthropic',
            'Claude': 'Claude',
            'GPT-4': 'GPT-4',
            'GPT-5': 'GPT-5',
            'GPT-4o': 'GPT-4o',
            'Gemini': 'Gemini',
            'Llama': 'Llama',
            'Mistral': 'Mistral',
            'DeepMind': 'DeepMind',
            'Google': 'Google',
            'Microsoft': 'Microsoft',
            'Meta': 'Meta',
            'NVIDIA': 'NVIDIA',
            'AI': 'AI',
            'LLM': 'LLM',
            # æ ¸å¿ƒæ¦‚å¿µç¿»è¯‘
            'artificial intelligence': 'äººå·¥æ™ºèƒ½',
            'machine learning': 'æœºå™¨å­¦ä¹ ',
            'deep learning': 'æ·±åº¦å­¦ä¹ ',
            'neural network': 'ç¥ç»ç½‘ç»œ',
            'large language model': 'å¤§è¯­è¨€æ¨¡å‹',
            'generative AI': 'ç”Ÿæˆå¼AI',
            'multimodal': 'å¤šæ¨¡æ€',
            'reasoning': 'æ¨ç†',
            'agent': 'æ™ºèƒ½ä½“',
            'autonomous': 'è‡ªä¸»',
            # åŠ¨ä½œç±»è¯æ±‡
            'breakthrough': 'é‡å¤§çªç ´',
            'launch': 'å‘å¸ƒ',
            'release': 'å‘å¸ƒ',
            'announce': 'å®£å¸ƒ',
            'unveil': 'æ­æ™“',
            'introduce': 'æ¨å‡º',
            'deploy': 'éƒ¨ç½²',
            'upgrade': 'å‡çº§',
            'update': 'æ›´æ–°',
            # å•†ä¸šç±»è¯æ±‡
            'partnership': 'åˆä½œ',
            'acquisition': 'æ”¶è´­',
            'funding': 'èèµ„',
            'investment': 'æŠ•èµ„',
            'valuation': 'ä¼°å€¼',
            'IPO': 'ä¸Šå¸‚',
            # æ”¿ç­–ç±»è¯æ±‡
            'regulation': 'ç›‘ç®¡',
            'policy': 'æ”¿ç­–',
            'legislation': 'ç«‹æ³•',
            'compliance': 'åˆè§„',
            'safety': 'å®‰å…¨',
            'ethics': 'ä¼¦ç†',
            # ç¨‹åº¦ç±»è¯æ±‡
            'major': 'é‡å¤§',
            'significant': 'é‡è¦',
            'critical': 'å…³é”®',
            'revolutionary': 'é©å‘½æ€§',
            'innovative': 'åˆ›æ–°',
            'first': 'é¦–æ¬¾',
            'new': 'æ–°',
            'latest': 'æœ€æ–°',
            # æŠ€æœ¯ç±»è¯æ±‡
            'open source': 'å¼€æº',
            'framework': 'æ¡†æ¶',
            'model': 'æ¨¡å‹',
            'architecture': 'æ¶æ„',
            'benchmark': 'åŸºå‡†æµ‹è¯•',
            'performance': 'æ€§èƒ½',
            'efficiency': 'æ•ˆç‡',
            'capability': 'èƒ½åŠ›',
            # è­¦ç¤ºç±»è¯æ±‡
            'warning': 'è­¦å‘Š',
            'alert': 'è­¦ç¤º',
            'concern': 'æ‹…å¿§',
            'risk': 'é£é™©',
        }
        
        # æ›´å…¨é¢çš„è‹±ä¸­ç¿»è¯‘è¯å…¸ï¼ˆç”¨äºæ ‡é¢˜å®Œæ•´ç¿»è¯‘ï¼‰
        self.full_translate_dict = {
            # å¸¸ç”¨åŠ¨è¯
            'is': 'æ˜¯', 'are': 'æ˜¯', 'was': 'æ˜¯', 'were': 'æ˜¯',
            'has': 'æœ‰', 'have': 'æœ‰', 'had': 'æœ‰',
            'will': 'å°†', 'would': 'å°†ä¼š', 'could': 'å¯ä»¥', 'can': 'èƒ½',
            'may': 'å¯èƒ½', 'might': 'å¯èƒ½',
            'gets': 'è·å¾—', 'get': 'è·å¾—', 'got': 'è·å¾—',
            'says': 'è¡¨ç¤º', 'say': 'è¡¨ç¤º', 'said': 'è¡¨ç¤º',
            'makes': 'åˆ¶ä½œ', 'make': 'åˆ¶ä½œ', 'made': 'åˆ¶ä½œ',
            'takes': 'é‡‡å–', 'take': 'é‡‡å–', 'took': 'é‡‡å–',
            'comes': 'æ¥', 'come': 'æ¥', 'came': 'æ¥',
            'goes': 'å»', 'go': 'å»', 'went': 'å»',
            'shows': 'å±•ç¤º', 'show': 'å±•ç¤º', 'showed': 'å±•ç¤º',
            'uses': 'ä½¿ç”¨', 'use': 'ä½¿ç”¨', 'used': 'ä½¿ç”¨',
            'brings': 'å¸¦æ¥', 'bring': 'å¸¦æ¥', 'brought': 'å¸¦æ¥',
            'becomes': 'æˆä¸º', 'become': 'æˆä¸º', 'became': 'æˆä¸º',
            'launches': 'å‘å¸ƒ', 'launched': 'å‘å¸ƒ',
            'releases': 'å‘å¸ƒ', 'released': 'å‘å¸ƒ',
            'announces': 'å®£å¸ƒ', 'announced': 'å®£å¸ƒ',
            'unveils': 'æ­ç¤º', 'unveiled': 'æ­ç¤º',
            'introduces': 'æ¨å‡º', 'introduced': 'æ¨å‡º',
            'reveals': 'æ­ç¤º', 'revealed': 'æ­ç¤º',
            'reports': 'æŠ¥é“', 'reported': 'æŠ¥é“',
            'claims': 'å£°ç§°', 'claimed': 'å£°ç§°',
            'confirms': 'ç¡®è®¤', 'confirmed': 'ç¡®è®¤',
            'denies': 'å¦è®¤', 'denied': 'å¦è®¤',
            'plans': 'è®¡åˆ’', 'planned': 'è®¡åˆ’',
            'aims': 'ç›®æ ‡', 'aimed': 'æ—¨åœ¨',
            'wants': 'æƒ³è¦', 'wanted': 'æƒ³è¦',
            'needs': 'éœ€è¦', 'needed': 'éœ€è¦',
            'builds': 'æ„å»º', 'build': 'æ„å»º', 'built': 'æ„å»º',
            'creates': 'åˆ›å»º', 'create': 'åˆ›å»º', 'created': 'åˆ›å»º',
            'develops': 'å¼€å‘', 'develop': 'å¼€å‘', 'developed': 'å¼€å‘',
            'trains': 'è®­ç»ƒ', 'train': 'è®­ç»ƒ', 'trained': 'è®­ç»ƒ',
            'tests': 'æµ‹è¯•', 'test': 'æµ‹è¯•', 'tested': 'æµ‹è¯•',
            'beats': 'å‡»è´¥', 'beat': 'å‡»è´¥',
            'wins': 'èµ¢å¾—', 'win': 'èµ¢å¾—', 'won': 'èµ¢å¾—',
            'loses': 'å¤±å»', 'lose': 'å¤±å»', 'lost': 'å¤±å»',
            'improves': 'æ”¹è¿›', 'improve': 'æ”¹è¿›', 'improved': 'æ”¹è¿›',
            'enables': 'ä½¿èƒ½', 'enable': 'ä½¿èƒ½', 'enabled': 'ä½¿èƒ½',
            'allows': 'å…è®¸', 'allow': 'å…è®¸', 'allowed': 'å…è®¸',
            'helps': 'å¸®åŠ©', 'help': 'å¸®åŠ©', 'helped': 'å¸®åŠ©',
            'works': 'å·¥ä½œ', 'work': 'å·¥ä½œ', 'worked': 'å·¥ä½œ',
            'runs': 'è¿è¡Œ', 'run': 'è¿è¡Œ', 'ran': 'è¿è¡Œ',
            'supports': 'æ”¯æŒ', 'support': 'æ”¯æŒ', 'supported': 'æ”¯æŒ',
            'offers': 'æä¾›', 'offer': 'æä¾›', 'offered': 'æä¾›',
            'provides': 'æä¾›', 'provide': 'æä¾›', 'provided': 'æä¾›',
            'adds': 'æ·»åŠ ', 'add': 'æ·»åŠ ', 'added': 'æ·»åŠ ',
            'removes': 'ç§»é™¤', 'remove': 'ç§»é™¤', 'removed': 'ç§»é™¤',
            'changes': 'æ”¹å˜', 'change': 'æ”¹å˜', 'changed': 'æ”¹å˜',
            'replaces': 'æ›¿æ¢', 'replace': 'æ›¿æ¢', 'replaced': 'æ›¿æ¢',
            'expands': 'æ‰©å±•', 'expand': 'æ‰©å±•', 'expanded': 'æ‰©å±•',
            'extends': 'æ‰©å±•', 'extend': 'æ‰©å±•', 'extended': 'æ‰©å±•',
            'accelerates': 'åŠ é€Ÿ', 'accelerate': 'åŠ é€Ÿ', 'accelerated': 'åŠ é€Ÿ',
            'slows': 'å‡ç¼“', 'slow': 'å‡ç¼“', 'slowed': 'å‡ç¼“',
            'starts': 'å¼€å§‹', 'start': 'å¼€å§‹', 'started': 'å¼€å§‹',
            'stops': 'åœæ­¢', 'stop': 'åœæ­¢', 'stopped': 'åœæ­¢',
            'ends': 'ç»“æŸ', 'end': 'ç»“æŸ', 'ended': 'ç»“æŸ',
            'begins': 'å¼€å§‹', 'begin': 'å¼€å§‹', 'began': 'å¼€å§‹',
            'continues': 'ç»§ç»­', 'continue': 'ç»§ç»­', 'continued': 'ç»§ç»­',
            'faces': 'é¢ä¸´', 'face': 'é¢ä¸´', 'faced': 'é¢ä¸´',
            'raises': 'æé«˜', 'raise': 'æé«˜', 'raised': 'æé«˜',
            'cuts': 'å‰Šå‡', 'cut': 'å‰Šå‡',
            'hits': 'è¾¾åˆ°', 'hit': 'è¾¾åˆ°',
            'reaches': 'è¾¾åˆ°', 'reach': 'è¾¾åˆ°', 'reached': 'è¾¾åˆ°',
            'grows': 'å¢é•¿', 'grow': 'å¢é•¿', 'grew': 'å¢é•¿',
            'falls': 'ä¸‹é™', 'fall': 'ä¸‹é™', 'fell': 'ä¸‹é™',
            'rises': 'ä¸Šå‡', 'rise': 'ä¸Šå‡', 'rose': 'ä¸Šå‡',
            'drops': 'ä¸‹é™', 'drop': 'ä¸‹é™', 'dropped': 'ä¸‹é™',
            'jumps': 'è·³è·ƒ', 'jump': 'è·³è·ƒ', 'jumped': 'è·³è·ƒ',
            'surges': 'æ¿€å¢', 'surge': 'æ¿€å¢', 'surged': 'æ¿€å¢',
            'soars': 'é£™å‡', 'soar': 'é£™å‡', 'soared': 'é£™å‡',
            'plunges': 'æš´è·Œ', 'plunge': 'æš´è·Œ', 'plunged': 'æš´è·Œ',
            'dives': 'è·³æ°´', 'dive': 'è·³æ°´', 'dived': 'è·³æ°´',
            'crashes': 'å´©æºƒ', 'crash': 'å´©æºƒ', 'crashed': 'å´©æºƒ',
            'outperforms': 'è¶…è¶Š', 'outperform': 'è¶…è¶Š', 'outperformed': 'è¶…è¶Š',
            'surpasses': 'è¶…è¶Š', 'surpass': 'è¶…è¶Š', 'surpassed': 'è¶…è¶Š',
            'exceeds': 'è¶…è¿‡', 'exceed': 'è¶…è¿‡', 'exceeded': 'è¶…è¿‡',
            'matches': 'åŒ¹é…', 'match': 'åŒ¹é…', 'matched': 'åŒ¹é…',
            'competes': 'ç«äº‰', 'compete': 'ç«äº‰', 'competed': 'ç«äº‰',
            'challenges': 'æŒ‘æˆ˜', 'challenge': 'æŒ‘æˆ˜', 'challenged': 'æŒ‘æˆ˜',
            'threatens': 'å¨èƒ', 'threaten': 'å¨èƒ', 'threatened': 'å¨èƒ',
            'warns': 'è­¦å‘Š', 'warn': 'è­¦å‘Š', 'warned': 'è­¦å‘Š',
            'predicts': 'é¢„æµ‹', 'predict': 'é¢„æµ‹', 'predicted': 'é¢„æµ‹',
            'expects': 'é¢„è®¡', 'expect': 'é¢„è®¡', 'expected': 'é¢„è®¡',
            'believes': 'ç›¸ä¿¡', 'believe': 'ç›¸ä¿¡', 'believed': 'ç›¸ä¿¡',
            'thinks': 'è®¤ä¸º', 'think': 'è®¤ä¸º', 'thought': 'è®¤ä¸º',
            'knows': 'çŸ¥é“', 'know': 'çŸ¥é“', 'knew': 'çŸ¥é“',
            'sees': 'çœ‹åˆ°', 'see': 'çœ‹åˆ°', 'saw': 'çœ‹åˆ°',
            'finds': 'å‘ç°', 'find': 'å‘ç°', 'found': 'å‘ç°',
            'discovers': 'å‘ç°', 'discover': 'å‘ç°', 'discovered': 'å‘ç°',
            'learns': 'å­¦ä¹ ', 'learn': 'å­¦ä¹ ', 'learned': 'å­¦ä¹ ',
            'teaches': 'æ•™', 'teach': 'æ•™', 'taught': 'æ•™',
            'writes': 'å†™', 'write': 'å†™', 'wrote': 'å†™',
            'reads': 'è¯»', 'read': 'è¯»',
            'speaks': 'è¯´', 'speak': 'è¯´', 'spoke': 'è¯´',
            'tells': 'å‘Šè¯‰', 'tell': 'å‘Šè¯‰', 'told': 'å‘Šè¯‰',
            'asks': 'è¯¢é—®', 'ask': 'è¯¢é—®', 'asked': 'è¯¢é—®',
            'answers': 'å›ç­”', 'answer': 'å›ç­”', 'answered': 'å›ç­”',
            'explains': 'è§£é‡Š', 'explain': 'è§£é‡Š', 'explained': 'è§£é‡Š',
            'describes': 'æè¿°', 'describe': 'æè¿°', 'described': 'æè¿°',
            'argues': 'äº‰è®º', 'argue': 'äº‰è®º', 'argued': 'äº‰è®º',
            'suggests': 'å»ºè®®', 'suggest': 'å»ºè®®', 'suggested': 'å»ºè®®',
            'recommends': 'æ¨è', 'recommend': 'æ¨è', 'recommended': 'æ¨è',
            'proposes': 'æè®®', 'propose': 'æè®®', 'proposed': 'æè®®',
            'considers': 'è€ƒè™‘', 'consider': 'è€ƒè™‘', 'considered': 'è€ƒè™‘',
            'explores': 'æ¢ç´¢', 'explore': 'æ¢ç´¢', 'explored': 'æ¢ç´¢',
            'investigates': 'è°ƒæŸ¥', 'investigate': 'è°ƒæŸ¥', 'investigated': 'è°ƒæŸ¥',
            'analyzes': 'åˆ†æ', 'analyze': 'åˆ†æ', 'analyzed': 'åˆ†æ',
            'evaluates': 'è¯„ä¼°', 'evaluate': 'è¯„ä¼°', 'evaluated': 'è¯„ä¼°',
            'assesses': 'è¯„ä¼°', 'assess': 'è¯„ä¼°', 'assessed': 'è¯„ä¼°',
            'measures': 'æµ‹é‡', 'measure': 'æµ‹é‡', 'measured': 'æµ‹é‡',
            'compares': 'æ¯”è¾ƒ', 'compare': 'æ¯”è¾ƒ', 'compared': 'æ¯”è¾ƒ',
            'combines': 'ç»“åˆ', 'combine': 'ç»“åˆ', 'combined': 'ç»“åˆ',
            'integrates': 'æ•´åˆ', 'integrate': 'æ•´åˆ', 'integrated': 'æ•´åˆ',
            'merges': 'åˆå¹¶', 'merge': 'åˆå¹¶', 'merged': 'åˆå¹¶',
            'acquires': 'æ”¶è´­', 'acquire': 'æ”¶è´­', 'acquired': 'æ”¶è´­',
            'buys': 'è´­ä¹°', 'buy': 'è´­ä¹°', 'bought': 'è´­ä¹°',
            'sells': 'å‡ºå”®', 'sell': 'å‡ºå”®', 'sold': 'å‡ºå”®',
            'invests': 'æŠ•èµ„', 'invest': 'æŠ•èµ„', 'invested': 'æŠ•èµ„',
            'funds': 'èµ„åŠ©', 'fund': 'èµ„åŠ©', 'funded': 'èµ„åŠ©',
            'partners': 'åˆä½œ', 'partner': 'åˆä½œ', 'partnered': 'åˆä½œ',
            'collaborates': 'åä½œ', 'collaborate': 'åä½œ', 'collaborated': 'åä½œ',
            'joins': 'åŠ å…¥', 'join': 'åŠ å…¥', 'joined': 'åŠ å…¥',
            'leaves': 'ç¦»å¼€', 'leave': 'ç¦»å¼€', 'left': 'ç¦»å¼€',
            'hires': 'æ‹›è˜', 'hire': 'æ‹›è˜', 'hired': 'æ‹›è˜',
            'fires': 'è§£é›‡', 'fire': 'è§£é›‡', 'fired': 'è§£é›‡',
            'appoints': 'ä»»å‘½', 'appoint': 'ä»»å‘½', 'appointed': 'ä»»å‘½',
            'names': 'å‘½å', 'name': 'å‘½å', 'named': 'å‘½å',
            'leads': 'é¢†å¯¼', 'lead': 'é¢†å¯¼', 'led': 'é¢†å¯¼',
            'follows': 'è·Ÿéš', 'follow': 'è·Ÿéš', 'followed': 'è·Ÿéš',
            'copies': 'å¤åˆ¶', 'copy': 'å¤åˆ¶', 'copied': 'å¤åˆ¶',
            'steals': 'çªƒå–', 'steal': 'çªƒå–', 'stole': 'çªƒå–',
            'sues': 'èµ·è¯‰', 'sue': 'èµ·è¯‰', 'sued': 'èµ·è¯‰',
            'bans': 'ç¦æ­¢', 'ban': 'ç¦æ­¢', 'banned': 'ç¦æ­¢',
            'blocks': 'é˜»æ­¢', 'block': 'é˜»æ­¢', 'blocked': 'é˜»æ­¢',
            'limits': 'é™åˆ¶', 'limit': 'é™åˆ¶', 'limited': 'é™åˆ¶',
            'restricts': 'é™åˆ¶', 'restrict': 'é™åˆ¶', 'restricted': 'é™åˆ¶',
            'regulates': 'ç›‘ç®¡', 'regulate': 'ç›‘ç®¡', 'regulated': 'ç›‘ç®¡',
            'controls': 'æ§åˆ¶', 'control': 'æ§åˆ¶', 'controlled': 'æ§åˆ¶',
            'manages': 'ç®¡ç†', 'manage': 'ç®¡ç†', 'managed': 'ç®¡ç†',
            'handles': 'å¤„ç†', 'handle': 'å¤„ç†', 'handled': 'å¤„ç†',
            'solves': 'è§£å†³', 'solve': 'è§£å†³', 'solved': 'è§£å†³',
            'fixes': 'ä¿®å¤', 'fix': 'ä¿®å¤', 'fixed': 'ä¿®å¤',
            'addresses': 'è§£å†³', 'address': 'è§£å†³', 'addressed': 'è§£å†³',
            'tackles': 'è§£å†³', 'tackle': 'è§£å†³', 'tackled': 'è§£å†³',
            'overcomes': 'å…‹æœ', 'overcome': 'å…‹æœ', 'overcame': 'å…‹æœ',
            'achieves': 'å®ç°', 'achieve': 'å®ç°', 'achieved': 'å®ç°',
            'accomplishes': 'å®Œæˆ', 'accomplish': 'å®Œæˆ', 'accomplished': 'å®Œæˆ',
            'completes': 'å®Œæˆ', 'complete': 'å®Œæˆ', 'completed': 'å®Œæˆ',
            'finishes': 'å®Œæˆ', 'finish': 'å®Œæˆ', 'finished': 'å®Œæˆ',
            'delivers': 'äº¤ä»˜', 'deliver': 'äº¤ä»˜', 'delivered': 'äº¤ä»˜',
            'ships': 'å‘å¸ƒ', 'ship': 'å‘å¸ƒ', 'shipped': 'å‘å¸ƒ',
            'rolls': 'æ¨å‡º', 'roll': 'æ¨å‡º', 'rolled': 'æ¨å‡º',
            'pushes': 'æ¨åŠ¨', 'push': 'æ¨åŠ¨', 'pushed': 'æ¨åŠ¨',
            'pulls': 'æ‹‰', 'pull': 'æ‹‰', 'pulled': 'æ‹‰',
            'drives': 'é©±åŠ¨', 'drive': 'é©±åŠ¨', 'drove': 'é©±åŠ¨',
            'powers': 'é©±åŠ¨', 'power': 'é©±åŠ¨', 'powered': 'é©±åŠ¨',
            'fuels': 'æ¨åŠ¨', 'fuel': 'æ¨åŠ¨', 'fueled': 'æ¨åŠ¨',
            'sparks': 'å¼•å‘', 'spark': 'å¼•å‘', 'sparked': 'å¼•å‘',
            'triggers': 'è§¦å‘', 'trigger': 'è§¦å‘', 'triggered': 'è§¦å‘',
            'causes': 'å¯¼è‡´', 'cause': 'å¯¼è‡´', 'caused': 'å¯¼è‡´',
            'leads': 'å¯¼è‡´', 'lead': 'å¯¼è‡´', 'led': 'å¯¼è‡´',
            'results': 'å¯¼è‡´', 'result': 'å¯¼è‡´', 'resulted': 'å¯¼è‡´',
            'produces': 'äº§ç”Ÿ', 'produce': 'äº§ç”Ÿ', 'produced': 'äº§ç”Ÿ',
            'generates': 'ç”Ÿæˆ', 'generate': 'ç”Ÿæˆ', 'generated': 'ç”Ÿæˆ',
            'outputs': 'è¾“å‡º', 'output': 'è¾“å‡º',
            'inputs': 'è¾“å…¥', 'input': 'è¾“å…¥',
            'processes': 'å¤„ç†', 'process': 'å¤„ç†', 'processed': 'å¤„ç†',
            'transforms': 'è½¬æ¢', 'transform': 'è½¬æ¢', 'transformed': 'è½¬æ¢',
            'converts': 'è½¬æ¢', 'convert': 'è½¬æ¢', 'converted': 'è½¬æ¢',
            'translates': 'ç¿»è¯‘', 'translate': 'ç¿»è¯‘', 'translated': 'ç¿»è¯‘',
            'adapts': 'é€‚åº”', 'adapt': 'é€‚åº”', 'adapted': 'é€‚åº”',
            'adjusts': 'è°ƒæ•´', 'adjust': 'è°ƒæ•´', 'adjusted': 'è°ƒæ•´',
            'modifies': 'ä¿®æ”¹', 'modify': 'ä¿®æ”¹', 'modified': 'ä¿®æ”¹',
            'customizes': 'å®šåˆ¶', 'customize': 'å®šåˆ¶', 'customized': 'å®šåˆ¶',
            'optimizes': 'ä¼˜åŒ–', 'optimize': 'ä¼˜åŒ–', 'optimized': 'ä¼˜åŒ–',
            'enhances': 'å¢å¼º', 'enhance': 'å¢å¼º', 'enhanced': 'å¢å¼º',
            'boosts': 'æå‡', 'boost': 'æå‡', 'boosted': 'æå‡',
            'strengthens': 'åŠ å¼º', 'strengthen': 'åŠ å¼º', 'strengthened': 'åŠ å¼º',
            'weakens': 'å‰Šå¼±', 'weaken': 'å‰Šå¼±', 'weakened': 'å‰Šå¼±',
            'reduces': 'å‡å°‘', 'reduce': 'å‡å°‘', 'reduced': 'å‡å°‘',
            'decreases': 'å‡å°‘', 'decrease': 'å‡å°‘', 'decreased': 'å‡å°‘',
            'increases': 'å¢åŠ ', 'increase': 'å¢åŠ ', 'increased': 'å¢åŠ ',
            'doubles': 'ç¿»å€', 'double': 'ç¿»å€', 'doubled': 'ç¿»å€',
            'triples': 'ä¸‰å€', 'triple': 'ä¸‰å€', 'tripled': 'ä¸‰å€',
            'halves': 'å‡åŠ', 'halve': 'å‡åŠ', 'halved': 'å‡åŠ',
            'scales': 'æ‰©å±•', 'scale': 'æ‰©å±•', 'scaled': 'æ‰©å±•',
            'shrinks': 'ç¼©å°', 'shrink': 'ç¼©å°', 'shrank': 'ç¼©å°',
            'expands': 'æ‰©å¤§', 'expand': 'æ‰©å¤§',
            # å¸¸ç”¨åè¯
            'company': 'å…¬å¸', 'companies': 'å…¬å¸',
            'startup': 'åˆåˆ›å…¬å¸', 'startups': 'åˆåˆ›å…¬å¸',
            'firm': 'å…¬å¸', 'firms': 'å…¬å¸',
            'corporation': 'ä¼ä¸š', 'corporations': 'ä¼ä¸š',
            'business': 'ä¸šåŠ¡', 'businesses': 'ä¸šåŠ¡',
            'industry': 'è¡Œä¸š', 'industries': 'è¡Œä¸š',
            'market': 'å¸‚åœº', 'markets': 'å¸‚åœº',
            'sector': 'é¢†åŸŸ', 'sectors': 'é¢†åŸŸ',
            'field': 'é¢†åŸŸ', 'fields': 'é¢†åŸŸ',
            'area': 'é¢†åŸŸ', 'areas': 'é¢†åŸŸ',
            'domain': 'é¢†åŸŸ', 'domains': 'é¢†åŸŸ',
            'technology': 'æŠ€æœ¯', 'technologies': 'æŠ€æœ¯',
            'tech': 'ç§‘æŠ€',
            'tool': 'å·¥å…·', 'tools': 'å·¥å…·',
            'product': 'äº§å“', 'products': 'äº§å“',
            'service': 'æœåŠ¡', 'services': 'æœåŠ¡',
            'platform': 'å¹³å°', 'platforms': 'å¹³å°',
            'system': 'ç³»ç»Ÿ', 'systems': 'ç³»ç»Ÿ',
            'software': 'è½¯ä»¶',
            'hardware': 'ç¡¬ä»¶',
            'application': 'åº”ç”¨', 'applications': 'åº”ç”¨',
            'app': 'åº”ç”¨', 'apps': 'åº”ç”¨',
            'feature': 'åŠŸèƒ½', 'features': 'åŠŸèƒ½',
            'function': 'åŠŸèƒ½', 'functions': 'åŠŸèƒ½',
            'ability': 'èƒ½åŠ›', 'abilities': 'èƒ½åŠ›',
            'skill': 'æŠ€èƒ½', 'skills': 'æŠ€èƒ½',
            'task': 'ä»»åŠ¡', 'tasks': 'ä»»åŠ¡',
            'job': 'å·¥ä½œ', 'jobs': 'å·¥ä½œ',
            'role': 'è§’è‰²', 'roles': 'è§’è‰²',
            'user': 'ç”¨æˆ·', 'users': 'ç”¨æˆ·',
            'customer': 'å®¢æˆ·', 'customers': 'å®¢æˆ·',
            'developer': 'å¼€å‘è€…', 'developers': 'å¼€å‘è€…',
            'researcher': 'ç ”ç©¶äººå‘˜', 'researchers': 'ç ”ç©¶äººå‘˜',
            'scientist': 'ç§‘å­¦å®¶', 'scientists': 'ç§‘å­¦å®¶',
            'engineer': 'å·¥ç¨‹å¸ˆ', 'engineers': 'å·¥ç¨‹å¸ˆ',
            'expert': 'ä¸“å®¶', 'experts': 'ä¸“å®¶',
            'leader': 'é¢†å¯¼è€…', 'leaders': 'é¢†å¯¼è€…',
            'CEO': 'é¦–å¸­æ‰§è¡Œå®˜', 'CTO': 'é¦–å¸­æŠ€æœ¯å®˜', 'CFO': 'é¦–å¸­è´¢åŠ¡å®˜',
            'founder': 'åˆ›å§‹äºº', 'founders': 'åˆ›å§‹äºº',
            'cofounder': 'è”åˆåˆ›å§‹äºº', 'cofounders': 'è”åˆåˆ›å§‹äºº',
            'team': 'å›¢é˜Ÿ', 'teams': 'å›¢é˜Ÿ',
            'group': 'é›†å›¢', 'groups': 'é›†å›¢',
            'organization': 'ç»„ç»‡', 'organizations': 'ç»„ç»‡',
            'government': 'æ”¿åºœ', 'governments': 'æ”¿åºœ',
            'agency': 'æœºæ„', 'agencies': 'æœºæ„',
            'institution': 'æœºæ„', 'institutions': 'æœºæ„',
            'university': 'å¤§å­¦', 'universities': 'å¤§å­¦',
            'lab': 'å®éªŒå®¤', 'labs': 'å®éªŒå®¤',
            'laboratory': 'å®éªŒå®¤', 'laboratories': 'å®éªŒå®¤',
            'center': 'ä¸­å¿ƒ', 'centers': 'ä¸­å¿ƒ',
            'institute': 'ç ”ç©¶æ‰€', 'institutes': 'ç ”ç©¶æ‰€',
            'research': 'ç ”ç©¶',
            'study': 'ç ”ç©¶', 'studies': 'ç ”ç©¶',
            'paper': 'è®ºæ–‡', 'papers': 'è®ºæ–‡',
            'report': 'æŠ¥å‘Š', 'reports': 'æŠ¥å‘Š',
            'article': 'æ–‡ç« ', 'articles': 'æ–‡ç« ',
            'blog': 'åšå®¢', 'blogs': 'åšå®¢',
            'post': 'å¸–å­', 'posts': 'å¸–å­',
            'news': 'æ–°é—»',
            'announcement': 'å…¬å‘Š', 'announcements': 'å…¬å‘Š',
            'statement': 'å£°æ˜', 'statements': 'å£°æ˜',
            'interview': 'é‡‡è®¿', 'interviews': 'é‡‡è®¿',
            'speech': 'æ¼”è®²', 'speeches': 'æ¼”è®²',
            'presentation': 'æ¼”ç¤º', 'presentations': 'æ¼”ç¤º',
            'demo': 'æ¼”ç¤º', 'demos': 'æ¼”ç¤º',
            'showcase': 'å±•ç¤º', 'showcases': 'å±•ç¤º',
            'event': 'æ´»åŠ¨', 'events': 'æ´»åŠ¨',
            'conference': 'ä¼šè®®', 'conferences': 'ä¼šè®®',
            'summit': 'å³°ä¼š', 'summits': 'å³°ä¼š',
            'meeting': 'ä¼šè®®', 'meetings': 'ä¼šè®®',
            'deal': 'äº¤æ˜“', 'deals': 'äº¤æ˜“',
            'agreement': 'åè®®', 'agreements': 'åè®®',
            'contract': 'åˆåŒ', 'contracts': 'åˆåŒ',
            'license': 'è®¸å¯', 'licenses': 'è®¸å¯',
            'patent': 'ä¸“åˆ©', 'patents': 'ä¸“åˆ©',
            'copyright': 'ç‰ˆæƒ', 'copyrights': 'ç‰ˆæƒ',
            'lawsuit': 'è¯‰è®¼', 'lawsuits': 'è¯‰è®¼',
            'case': 'æ¡ˆä¾‹', 'cases': 'æ¡ˆä¾‹',
            'issue': 'é—®é¢˜', 'issues': 'é—®é¢˜',
            'problem': 'é—®é¢˜', 'problems': 'é—®é¢˜',
            'challenge': 'æŒ‘æˆ˜', 'challenges': 'æŒ‘æˆ˜',
            'opportunity': 'æœºä¼š', 'opportunities': 'æœºä¼š',
            'threat': 'å¨èƒ', 'threats': 'å¨èƒ',
            'risk': 'é£é™©', 'risks': 'é£é™©',
            'danger': 'å±é™©', 'dangers': 'å±é™©',
            'concern': 'æ‹…å¿§', 'concerns': 'æ‹…å¿§',
            'worry': 'æ‹…å¿§', 'worries': 'æ‹…å¿§',
            'fear': 'ææƒ§', 'fears': 'ææƒ§',
            'hope': 'å¸Œæœ›', 'hopes': 'å¸Œæœ›',
            'dream': 'æ¢¦æƒ³', 'dreams': 'æ¢¦æƒ³',
            'vision': 'æ„¿æ™¯', 'visions': 'æ„¿æ™¯',
            'goal': 'ç›®æ ‡', 'goals': 'ç›®æ ‡',
            'target': 'ç›®æ ‡', 'targets': 'ç›®æ ‡',
            'objective': 'ç›®æ ‡', 'objectives': 'ç›®æ ‡',
            'plan': 'è®¡åˆ’', 'plans': 'è®¡åˆ’',
            'strategy': 'æˆ˜ç•¥', 'strategies': 'æˆ˜ç•¥',
            'approach': 'æ–¹æ³•', 'approaches': 'æ–¹æ³•',
            'method': 'æ–¹æ³•', 'methods': 'æ–¹æ³•',
            'technique': 'æŠ€æœ¯', 'techniques': 'æŠ€æœ¯',
            'solution': 'è§£å†³æ–¹æ¡ˆ', 'solutions': 'è§£å†³æ–¹æ¡ˆ',
            'answer': 'ç­”æ¡ˆ', 'answers': 'ç­”æ¡ˆ',
            'response': 'å›åº”', 'responses': 'å›åº”',
            'reaction': 'ååº”', 'reactions': 'ååº”',
            'feedback': 'åé¦ˆ',
            'review': 'è¯„æµ‹', 'reviews': 'è¯„æµ‹',
            'rating': 'è¯„åˆ†', 'ratings': 'è¯„åˆ†',
            'score': 'å¾—åˆ†', 'scores': 'å¾—åˆ†',
            'result': 'ç»“æœ', 'results': 'ç»“æœ',
            'outcome': 'ç»“æœ', 'outcomes': 'ç»“æœ',
            'effect': 'æ•ˆæœ', 'effects': 'æ•ˆæœ',
            'impact': 'å½±å“', 'impacts': 'å½±å“',
            'influence': 'å½±å“', 'influences': 'å½±å“',
            'change': 'å˜åŒ–', 'changes': 'å˜åŒ–',
            'shift': 'è½¬å˜', 'shifts': 'è½¬å˜',
            'transition': 'è¿‡æ¸¡', 'transitions': 'è¿‡æ¸¡',
            'transformation': 'è½¬å‹', 'transformations': 'è½¬å‹',
            'revolution': 'é©å‘½', 'revolutions': 'é©å‘½',
            'evolution': 'è¿›åŒ–', 'evolutions': 'è¿›åŒ–',
            'progress': 'è¿›å±•',
            'advance': 'è¿›å±•', 'advances': 'è¿›å±•',
            'advancement': 'è¿›æ­¥', 'advancements': 'è¿›æ­¥',
            'development': 'å‘å±•', 'developments': 'å‘å±•',
            'growth': 'å¢é•¿',
            'expansion': 'æ‰©å¼ ', 'expansions': 'æ‰©å¼ ',
            'trend': 'è¶‹åŠ¿', 'trends': 'è¶‹åŠ¿',
            'pattern': 'æ¨¡å¼', 'patterns': 'æ¨¡å¼',
            'cycle': 'å‘¨æœŸ', 'cycles': 'å‘¨æœŸ',
            'phase': 'é˜¶æ®µ', 'phases': 'é˜¶æ®µ',
            'stage': 'é˜¶æ®µ', 'stages': 'é˜¶æ®µ',
            'step': 'æ­¥éª¤', 'steps': 'æ­¥éª¤',
            'level': 'çº§åˆ«', 'levels': 'çº§åˆ«',
            'tier': 'å±‚', 'tiers': 'å±‚',
            'layer': 'å±‚', 'layers': 'å±‚',
            'version': 'ç‰ˆæœ¬', 'versions': 'ç‰ˆæœ¬',
            'edition': 'ç‰ˆæœ¬', 'editions': 'ç‰ˆæœ¬',
            'generation': 'ä»£', 'generations': 'ä»£',
            'era': 'æ—¶ä»£', 'eras': 'æ—¶ä»£',
            'age': 'æ—¶ä»£', 'ages': 'æ—¶ä»£',
            'future': 'æœªæ¥', 'futures': 'æœªæ¥',
            'past': 'è¿‡å»',
            'present': 'ç°åœ¨',
            'today': 'ä»Šå¤©',
            'tomorrow': 'æ˜å¤©',
            'yesterday': 'æ˜¨å¤©',
            'year': 'å¹´', 'years': 'å¹´',
            'month': 'æœˆ', 'months': 'æœˆ',
            'week': 'å‘¨', 'weeks': 'å‘¨',
            'day': 'å¤©', 'days': 'å¤©',
            'hour': 'å°æ—¶', 'hours': 'å°æ—¶',
            'minute': 'åˆ†é’Ÿ', 'minutes': 'åˆ†é’Ÿ',
            'second': 'ç§’', 'seconds': 'ç§’',
            'time': 'æ—¶é—´', 'times': 'æ—¶é—´',
            'world': 'ä¸–ç•Œ', 'worlds': 'ä¸–ç•Œ',
            'global': 'å…¨çƒ',
            'international': 'å›½é™…',
            'national': 'å›½å®¶',
            'local': 'æœ¬åœ°',
            'regional': 'åŒºåŸŸ',
            'country': 'å›½å®¶', 'countries': 'å›½å®¶',
            'nation': 'å›½å®¶', 'nations': 'å›½å®¶',
            'state': 'å·', 'states': 'å·',
            'city': 'åŸå¸‚', 'cities': 'åŸå¸‚',
            'region': 'åœ°åŒº', 'regions': 'åœ°åŒº',
            'billion': 'åäº¿',
            'million': 'ç™¾ä¸‡',
            'thousand': 'åƒ',
            'hundred': 'ç™¾',
            'percent': 'ç™¾åˆ†æ¯”',
            'dollar': 'ç¾å…ƒ', 'dollars': 'ç¾å…ƒ',
            'price': 'ä»·æ ¼', 'prices': 'ä»·æ ¼',
            'cost': 'æˆæœ¬', 'costs': 'æˆæœ¬',
            'value': 'ä»·å€¼', 'values': 'ä»·å€¼',
            'worth': 'ä»·å€¼',
            'revenue': 'æ”¶å…¥', 'revenues': 'æ”¶å…¥',
            'profit': 'åˆ©æ¶¦', 'profits': 'åˆ©æ¶¦',
            'loss': 'æŸå¤±', 'losses': 'æŸå¤±',
            'gain': 'æ”¶ç›Š', 'gains': 'æ”¶ç›Š',
            'return': 'å›æŠ¥', 'returns': 'å›æŠ¥',
            'income': 'æ”¶å…¥', 'incomes': 'æ”¶å…¥',
            'money': 'èµ„é‡‘',
            'cash': 'ç°é‡‘',
            'capital': 'èµ„æœ¬',
            'asset': 'èµ„äº§', 'assets': 'èµ„äº§',
            'debt': 'å€ºåŠ¡', 'debts': 'å€ºåŠ¡',
            'stock': 'è‚¡ç¥¨', 'stocks': 'è‚¡ç¥¨',
            'share': 'è‚¡ä»½', 'shares': 'è‚¡ä»½',
            'bond': 'å€ºåˆ¸', 'bonds': 'å€ºåˆ¸',
            'fund': 'åŸºé‡‘', 'funds': 'åŸºé‡‘',
            'round': 'è½®', 'rounds': 'è½®',
            'series': 'ç³»åˆ—',
            'seed': 'ç§å­',
            # AIç›¸å…³ä¸“ä¸šæœ¯è¯­
            'chatbot': 'èŠå¤©æœºå™¨äºº', 'chatbots': 'èŠå¤©æœºå™¨äºº',
            'bot': 'æœºå™¨äºº', 'bots': 'æœºå™¨äºº',
            'robot': 'æœºå™¨äºº', 'robots': 'æœºå™¨äºº',
            'robotics': 'æœºå™¨äººæŠ€æœ¯',
            'automation': 'è‡ªåŠ¨åŒ–',
            'algorithm': 'ç®—æ³•', 'algorithms': 'ç®—æ³•',
            'data': 'æ•°æ®',
            'dataset': 'æ•°æ®é›†', 'datasets': 'æ•°æ®é›†',
            'database': 'æ•°æ®åº“', 'databases': 'æ•°æ®åº“',
            'training': 'è®­ç»ƒ',
            'inference': 'æ¨ç†',
            'prediction': 'é¢„æµ‹', 'predictions': 'é¢„æµ‹',
            'classification': 'åˆ†ç±»',
            'recognition': 'è¯†åˆ«',
            'detection': 'æ£€æµ‹',
            'generation': 'ç”Ÿæˆ',
            'synthesis': 'åˆæˆ',
            'analysis': 'åˆ†æ',
            'processing': 'å¤„ç†',
            'understanding': 'ç†è§£',
            'learning': 'å­¦ä¹ ',
            'intelligence': 'æ™ºèƒ½',
            'cognition': 'è®¤çŸ¥',
            'perception': 'æ„ŸçŸ¥',
            'vision': 'è§†è§‰',
            'speech': 'è¯­éŸ³',
            'language': 'è¯­è¨€', 'languages': 'è¯­è¨€',
            'text': 'æ–‡æœ¬', 'texts': 'æ–‡æœ¬',
            'image': 'å›¾åƒ', 'images': 'å›¾åƒ',
            'video': 'è§†é¢‘', 'videos': 'è§†é¢‘',
            'audio': 'éŸ³é¢‘', 'audios': 'éŸ³é¢‘',
            'voice': 'è¯­éŸ³', 'voices': 'è¯­éŸ³',
            'sound': 'å£°éŸ³', 'sounds': 'å£°éŸ³',
            'code': 'ä»£ç ', 'codes': 'ä»£ç ',
            'coding': 'ç¼–ç¨‹',
            'programming': 'ç¼–ç¨‹',
            'prompt': 'æç¤ºè¯', 'prompts': 'æç¤ºè¯',
            'token': 'ä»¤ç‰Œ', 'tokens': 'ä»¤ç‰Œ',
            'parameter': 'å‚æ•°', 'parameters': 'å‚æ•°',
            'weight': 'æƒé‡', 'weights': 'æƒé‡',
            'layer': 'å±‚', 'layers': 'å±‚',
            'neuron': 'ç¥ç»å…ƒ', 'neurons': 'ç¥ç»å…ƒ',
            'attention': 'æ³¨æ„åŠ›',
            'transformer': 'Transformer',
            'encoder': 'ç¼–ç å™¨', 'encoders': 'ç¼–ç å™¨',
            'decoder': 'è§£ç å™¨', 'decoders': 'è§£ç å™¨',
            'embedding': 'åµŒå…¥', 'embeddings': 'åµŒå…¥',
            'vector': 'å‘é‡', 'vectors': 'å‘é‡',
            'matrix': 'çŸ©é˜µ', 'matrices': 'çŸ©é˜µ',
            'tensor': 'å¼ é‡', 'tensors': 'å¼ é‡',
            'GPU': 'GPU', 'GPUs': 'GPU',
            'CPU': 'CPU', 'CPUs': 'CPU',
            'chip': 'èŠ¯ç‰‡', 'chips': 'èŠ¯ç‰‡',
            'processor': 'å¤„ç†å™¨', 'processors': 'å¤„ç†å™¨',
            'server': 'æœåŠ¡å™¨', 'servers': 'æœåŠ¡å™¨',
            'cloud': 'äº‘',
            'edge': 'è¾¹ç¼˜',
            'device': 'è®¾å¤‡', 'devices': 'è®¾å¤‡',
            'computer': 'è®¡ç®—æœº', 'computers': 'è®¡ç®—æœº',
            'computing': 'è®¡ç®—',
            'memory': 'å†…å­˜',
            'storage': 'å­˜å‚¨',
            'bandwidth': 'å¸¦å®½',
            'latency': 'å»¶è¿Ÿ',
            'throughput': 'ååé‡',
            'speed': 'é€Ÿåº¦',
            'accuracy': 'å‡†ç¡®ç‡',
            'precision': 'ç²¾åº¦',
            'recall': 'å¬å›ç‡',
            'loss': 'æŸå¤±',
            'error': 'è¯¯å·®', 'errors': 'è¯¯å·®',
            'bias': 'åè§', 'biases': 'åè§',
            'fairness': 'å…¬å¹³æ€§',
            'transparency': 'é€æ˜åº¦',
            'explainability': 'å¯è§£é‡Šæ€§',
            'interpretability': 'å¯è§£é‡Šæ€§',
            'alignment': 'å¯¹é½',
            'safety': 'å®‰å…¨',
            'security': 'å®‰å…¨æ€§',
            'privacy': 'éšç§',
            'trust': 'ä¿¡ä»»',
            'reliability': 'å¯é æ€§',
            'robustness': 'é²æ£’æ€§',
            'scalability': 'å¯æ‰©å±•æ€§',
            'efficiency': 'æ•ˆç‡',
            'effectiveness': 'æœ‰æ•ˆæ€§',
            'quality': 'è´¨é‡',
            'quantity': 'æ•°é‡',
            'size': 'å¤§å°', 'sizes': 'å¤§å°',
            'scale': 'è§„æ¨¡', 'scales': 'è§„æ¨¡',
            'scope': 'èŒƒå›´', 'scopes': 'èŒƒå›´',
            'range': 'èŒƒå›´', 'ranges': 'èŒƒå›´',
            'limit': 'é™åˆ¶', 'limits': 'é™åˆ¶',
            'boundary': 'è¾¹ç•Œ', 'boundaries': 'è¾¹ç•Œ',
            'frontier': 'å‰æ²¿', 'frontiers': 'å‰æ²¿',
            'edge': 'è¾¹ç¼˜', 'edges': 'è¾¹ç¼˜',
            'core': 'æ ¸å¿ƒ', 'cores': 'æ ¸å¿ƒ',
            'base': 'åŸºç¡€', 'bases': 'åŸºç¡€',
            'foundation': 'åŸºç¡€', 'foundations': 'åŸºç¡€',
            'fundamental': 'åŸºæœ¬',
            'basic': 'åŸºæœ¬',
            'advanced': 'é«˜çº§',
            'sophisticated': 'å¤æ‚',
            'complex': 'å¤æ‚',
            'simple': 'ç®€å•',
            'easy': 'ç®€å•',
            'difficult': 'å›°éš¾',
            'hard': 'å›°éš¾',
            'challenging': 'å…·æœ‰æŒ‘æˆ˜æ€§',
            'impossible': 'ä¸å¯èƒ½',
            'possible': 'å¯èƒ½',
            'likely': 'å¯èƒ½',
            'unlikely': 'ä¸å¤ªå¯èƒ½',
            'certain': 'ç¡®å®š',
            'uncertain': 'ä¸ç¡®å®š',
            'clear': 'æ¸…æ¥š',
            'unclear': 'ä¸æ¸…æ¥š',
            'obvious': 'æ˜æ˜¾',
            'subtle': 'å¾®å¦™',
            'significant': 'é‡å¤§',
            'important': 'é‡è¦',
            'critical': 'å…³é”®',
            'essential': 'å¿…è¦',
            'necessary': 'å¿…è¦',
            'optional': 'å¯é€‰',
            'required': 'å¿…éœ€',
            'mandatory': 'å¼ºåˆ¶',
            'voluntary': 'è‡ªæ„¿',
            'free': 'å…è´¹',
            'paid': 'ä»˜è´¹',
            'premium': 'é«˜çº§',
            'standard': 'æ ‡å‡†',
            'custom': 'è‡ªå®šä¹‰',
            'default': 'é»˜è®¤',
            'official': 'å®˜æ–¹',
            'unofficial': 'éå®˜æ–¹',
            'public': 'å…¬å¼€',
            'private': 'ç§æœ‰',
            'open': 'å¼€æ”¾',
            'closed': 'å°é—­',
            'available': 'å¯ç”¨',
            'unavailable': 'ä¸å¯ç”¨',
            'online': 'åœ¨çº¿',
            'offline': 'ç¦»çº¿',
            'live': 'å®æ—¶',
            'real-time': 'å®æ—¶',
            'instant': 'å³æ—¶',
            'fast': 'å¿«é€Ÿ',
            'slow': 'ç¼“æ…¢',
            'quick': 'å¿«é€Ÿ',
            'rapid': 'å¿«é€Ÿ',
            'immediate': 'ç«‹å³',
            'soon': 'å³å°†',
            'next': 'ä¸‹ä¸€ä¸ª',
            'previous': 'ä¸Šä¸€ä¸ª',
            'current': 'å½“å‰',
            'former': 'å‰',
            'latter': 'åè€…',
            'first': 'é¦–ä¸ª',
            'last': 'æœ€å',
            'final': 'æœ€ç»ˆ',
            'initial': 'åˆå§‹',
            'original': 'åŸå§‹',
            'updated': 'æ›´æ–°çš„',
            'improved': 'æ”¹è¿›çš„',
            'enhanced': 'å¢å¼ºçš„',
            'upgraded': 'å‡çº§çš„',
            'new': 'æ–°çš„',
            'old': 'æ—§çš„',
            'modern': 'ç°ä»£',
            'traditional': 'ä¼ ç»Ÿ',
            'classic': 'ç»å…¸',
            'novel': 'æ–°é¢–',
            'unique': 'ç‹¬ç‰¹',
            'special': 'ç‰¹æ®Š',
            'general': 'é€šç”¨',
            'specific': 'ç‰¹å®š',
            'particular': 'ç‰¹å®š',
            'individual': 'ä¸ªäºº',
            'personal': 'ä¸ªäºº',
            'professional': 'ä¸“ä¸š',
            'commercial': 'å•†ä¸š',
            'enterprise': 'ä¼ä¸š',
            'consumer': 'æ¶ˆè´¹è€…',
            'retail': 'é›¶å”®',
            'wholesale': 'æ‰¹å‘',
            'domestic': 'å›½å†…',
            'foreign': 'å¤–å›½',
            'overseas': 'æµ·å¤–',
            'worldwide': 'å…¨çƒ',
            # å¸¸ç”¨ä»‹è¯å’Œè¿è¯
            'the': '', 'a': '', 'an': '',
            'of': 'çš„', 'for': 'ä¸º', 'to': 'åˆ°', 'from': 'ä»',
            'in': 'åœ¨', 'on': 'åœ¨', 'at': 'åœ¨',
            'by': 'ç”±', 'with': 'ä¸', 'without': 'æ²¡æœ‰',
            'about': 'å…³äº', 'around': 'çº¦', 'between': 'ä¹‹é—´',
            'through': 'é€šè¿‡', 'during': 'æœŸé—´', 'before': 'ä¹‹å‰', 'after': 'ä¹‹å',
            'above': 'ä»¥ä¸Š', 'below': 'ä»¥ä¸‹', 'under': 'ä¸‹',
            'over': 'è¶…è¿‡', 'into': 'è¿›å…¥', 'out': 'å‡º',
            'up': 'ä¸Š', 'down': 'ä¸‹',
            'and': 'å’Œ', 'or': 'æˆ–', 'but': 'ä½†',
            'if': 'å¦‚æœ', 'then': 'é‚£ä¹ˆ', 'else': 'å¦åˆ™',
            'when': 'å½“', 'where': 'å“ªé‡Œ', 'why': 'ä¸ºä»€ä¹ˆ', 'how': 'å¦‚ä½•',
            'what': 'ä»€ä¹ˆ', 'which': 'å“ªä¸ª', 'who': 'è°', 'whom': 'è°',
            'that': 'é‚£ä¸ª', 'this': 'è¿™ä¸ª', 'these': 'è¿™äº›', 'those': 'é‚£äº›',
            'all': 'æ‰€æœ‰', 'some': 'ä¸€äº›', 'any': 'ä»»ä½•', 'no': 'æ²¡æœ‰',
            'every': 'æ¯ä¸ª', 'each': 'æ¯ä¸ª', 'both': 'ä¸¤è€…', 'either': 'ä»»ä¸€',
            'neither': 'ä¸¤è€…éƒ½ä¸', 'none': 'æ²¡æœ‰',
            'more': 'æ›´å¤š', 'less': 'æ›´å°‘', 'most': 'æœ€', 'least': 'æœ€å°‘',
            'much': 'å¾ˆå¤š', 'many': 'å¾ˆå¤š', 'few': 'å¾ˆå°‘', 'little': 'å¾ˆå°‘',
            'very': 'éå¸¸', 'too': 'å¤ª', 'so': 'å¦‚æ­¤', 'such': 'å¦‚æ­¤',
            'quite': 'ç›¸å½“', 'rather': 'ç›¸å½“', 'fairly': 'ç›¸å½“',
            'really': 'çœŸçš„', 'actually': 'å®é™…ä¸Š', 'basically': 'åŸºæœ¬ä¸Š',
            'especially': 'ç‰¹åˆ«æ˜¯', 'particularly': 'ç‰¹åˆ«æ˜¯',
            'mainly': 'ä¸»è¦', 'mostly': 'ä¸»è¦', 'largely': 'å¤§éƒ¨åˆ†',
            'entirely': 'å®Œå…¨', 'completely': 'å®Œå…¨', 'totally': 'å®Œå…¨',
            'fully': 'å®Œå…¨', 'partly': 'éƒ¨åˆ†', 'partially': 'éƒ¨åˆ†',
            'almost': 'å‡ ä¹', 'nearly': 'å‡ ä¹', 'just': 'åˆšåˆš',
            'only': 'åª', 'even': 'ç”šè‡³', 'still': 'ä»ç„¶', 'yet': 'è¿˜',
            'already': 'å·²ç»', 'now': 'ç°åœ¨', 'then': 'ç„¶å',
            'here': 'è¿™é‡Œ', 'there': 'é‚£é‡Œ', 'everywhere': 'åˆ°å¤„',
            'somewhere': 'æŸå¤„', 'nowhere': 'æ— å¤„', 'anywhere': 'ä»»ä½•åœ°æ–¹',
            'also': 'ä¹Ÿ', 'again': 'å†æ¬¡', 'always': 'æ€»æ˜¯', 'never': 'ä»ä¸',
            'often': 'ç»å¸¸', 'sometimes': 'æœ‰æ—¶', 'usually': 'é€šå¸¸',
            'rarely': 'å¾ˆå°‘', 'seldom': 'å¾ˆå°‘',
            'perhaps': 'ä¹Ÿè®¸', 'maybe': 'ä¹Ÿè®¸', 'probably': 'å¯èƒ½',
            'certainly': 'å½“ç„¶', 'definitely': 'è‚¯å®š', 'surely': 'è‚¯å®š',
            'however': 'ç„¶è€Œ', 'therefore': 'å› æ­¤', 'thus': 'å› æ­¤',
            'hence': 'å› æ­¤', 'meanwhile': 'åŒæ—¶', 'furthermore': 'æ­¤å¤–',
            'moreover': 'è€Œä¸”', 'besides': 'æ­¤å¤–', 'otherwise': 'å¦åˆ™',
            'instead': 'è€Œæ˜¯', 'rather': 'å®æ„¿', 'despite': 'å°½ç®¡',
            'although': 'è™½ç„¶', 'though': 'å°½ç®¡', 'while': 'å½“', 'whereas': 'ç„¶è€Œ',
            'unless': 'é™¤é', 'until': 'ç›´åˆ°', 'since': 'è‡ªä»', 'because': 'å› ä¸º',
            'as': 'ä½œä¸º', 'like': 'åƒ', 'unlike': 'ä¸åƒ',
            'according': 'æ ¹æ®', 'regarding': 'å…³äº', 'concerning': 'å…³äº',
            'including': 'åŒ…æ‹¬', 'excluding': 'ä¸åŒ…æ‹¬',
            'following': 'ä»¥ä¸‹', 'considering': 'è€ƒè™‘åˆ°',
            # è¡¥å……é—æ¼çš„å¸¸ç”¨è¯
            'course': 'è·¯çº¿', 'courses': 'è·¯çº¿',
            'charts': 'è§„åˆ’', 'chart': 'è§„åˆ’', 'charted': 'è§„åˆ’',
            'flagship': 'æ——èˆ°',
            'sales': 'é”€å”®',
            'momentum': 'åŠ¿å¤´',
            'propel': 'æ¨åŠ¨', 'propels': 'æ¨åŠ¨', 'propelled': 'æ¨åŠ¨',
            'ambitions': 'é›„å¿ƒ', 'ambition': 'é›„å¿ƒ',
            'seeks': 'å¯»æ±‚', 'seek': 'å¯»æ±‚', 'sought': 'å¯»æ±‚',
            'federal': 'è”é‚¦',
            'approval': 'æ‰¹å‡†', 'approvals': 'æ‰¹å‡†',
            'solar': 'å¤ªé˜³èƒ½',
            'powered': 'é©±åŠ¨çš„',
            'satellite': 'å«æ˜Ÿ', 'satellites': 'å«æ˜Ÿ',
            'data center': 'æ•°æ®ä¸­å¿ƒ', 'data centers': 'æ•°æ®ä¸­å¿ƒ',
            'weather': 'å¤©æ°”',
            'forecast': 'é¢„æµ‹', 'forecasts': 'é¢„æµ‹', 'forecasting': 'é¢„æµ‹',
            'faster': 'æ›´å¿«',
            'cheaper': 'æ›´ä¾¿å®œ',
            'red': 'çº¢', 'blue': 'è“',
            'team': 'å›¢é˜Ÿ', 'teams': 'å›¢é˜Ÿ',
            'test': 'æµ‹è¯•', 'tests': 'æµ‹è¯•', 'tested': 'æµ‹è¯•',
            'ran': 'è¿è¡Œ',
            'AMA': 'é—®ç­”',
            'our': 'æˆ‘ä»¬çš„',
            'their': 'ä»–ä»¬çš„',
            'its': 'å®ƒçš„',
            'your': 'ä½ çš„',
            'my': 'æˆ‘çš„',
            'his': 'ä»–çš„',
            'her': 'å¥¹çš„',
            'rumored': 'ä¼ é—»', 'rumor': 'ä¼ é—»', 'rumors': 'ä¼ é—»',
            'merger': 'åˆå¹¶', 'mergers': 'åˆå¹¶',
            'apparent': 'æ˜æ˜¾',
            'confirmation': 'ç¡®è®¤',
            'selects': 'é€‰æ‹©', 'select': 'é€‰æ‹©', 'selected': 'é€‰æ‹©',
            'build': 'æ„å»º', 'builds': 'æ„å»º', 'built': 'æ„å»º',
            'design': 'è®¾è®¡', 'designs': 'è®¾è®¡', 'designed': 'è®¾è®¡',
            'using': 'ä½¿ç”¨',
            'eight': 'å…«',
            'billion': 'åäº¿',
            'parameter': 'å‚æ•°', 'parameters': 'å‚æ•°',
            'state': 'çŠ¶æ€', 'states': 'çŠ¶æ€',
            'space': 'ç©ºé—´', 'spaces': 'ç©ºé—´',
            'complaints': 'æŠ•è¯‰', 'complaint': 'æŠ•è¯‰',
            'megathread': 'è®¨è®ºå¸–',
            'vs': 'å¯¹æŠ—',
            'brewery': 'é…¿é…’å‚',
            'beverage': 'é¥®æ–™',
            'inbox': 'æ”¶ä»¶ç®±',
            'feel': 'æ„Ÿè§‰',
            'fraction': 'ä¸€å°éƒ¨åˆ†',
            'cost': 'æˆæœ¬', 'costs': 'æˆæœ¬',
            'demonstrating': 'å±•ç¤º',
            'viability': 'å¯è¡Œæ€§',
            'next-generation': 'ä¸‹ä¸€ä»£',
            # æ›´å¤šé—æ¼è¯æ±‡
            'models': 'æ¨¡å‹',
            'we': 'æˆ‘ä»¬',
            'they': 'ä»–ä»¬',
            'it': 'å®ƒ',
            'he': 'ä»–',
            'she': 'å¥¹',
            'I': 'æˆ‘',
            'you': 'ä½ ',
            'me': 'æˆ‘',
            'him': 'ä»–',
            'us': 'æˆ‘ä»¬',
            'them': 'ä»–ä»¬',
            'agents': 'æ™ºèƒ½ä½“',
            'live': 'å®æ—¶',
            'real': 'çœŸå®',
            'time': 'æ—¶é—´',
        }
        
        # === RSS Feed é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œç¨³å®šæ€§é«˜ï¼Œåçˆ¬é£é™©ä½ï¼‰===
        # ç‰¹åˆ«é€‚åˆ GitHub Actions ç¯å¢ƒ
        self.rss_feeds = {
            # === ä¸€çº§æƒå¨æ¥æºï¼ˆAIå…¬å¸å®˜æ–¹ã€é¡¶çº§ç§‘æŠ€åª’ä½“ï¼‰===
            'TechCrunch AI': {
                'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
                'authority': 8,
                'category': 'tech_media'
            },
            'Wired AI': {
                'url': 'https://www.wired.com/feed/tag/ai/latest/rss',
                'authority': 8,
                'category': 'tech_media'
            },
            'MIT Technology Review': {
                'url': 'https://www.technologyreview.com/feed/',
                'authority': 9,
                'category': 'tech_media'
            },
            'Ars Technica': {
                'url': 'https://feeds.arstechnica.com/arstechnica/technology-lab',
                'authority': 7,
                'category': 'tech_media'
            },
            
            # === Hacker Newsï¼ˆç¡…è°·é£å‘æ ‡ï¼‰===
            'Hacker News Front Page': {
                'url': 'https://hnrss.org/frontpage',
                'authority': 7,
                'category': 'community'
            },
            'Hacker News Best': {
                'url': 'https://hnrss.org/best',
                'authority': 7,
                'category': 'community'
            },
            
            # === å­¦æœ¯æ¥æº ===
            'arXiv AI': {
                'url': 'https://export.arxiv.org/rss/cs.AI',
                'authority': 9,
                'category': 'academic'
            },
            'arXiv Machine Learning': {
                'url': 'https://export.arxiv.org/rss/cs.LG',
                'authority': 9,
                'category': 'academic'
            },
            
            # === AIä¸“ä¸šåª’ä½“ ===
            'The Gradient': {
                'url': 'https://thegradient.pub/rss/',
                'authority': 8,
                'category': 'ai_media'
            },
            
            # === å¼€å‘è€…ç¤¾åŒº ===
            'Hugging Face Blog': {
                'url': 'https://huggingface.co/blog/feed.xml',
                'authority': 8,
                'category': 'ai_company'
            },
        }
    
    def log(self, message, level='INFO'):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_msg = f"[{timestamp}] [{level}] {message}\n"
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_msg)
        
        print(log_msg.strip())
    
    def fetch_rss_feeds(self):
        """
        ã€æ ¸å¿ƒæ–¹æ³•ã€‘ä»æ‰€æœ‰é…ç½®çš„RSSæºè·å–æ–°é—»
        ä¼˜åŠ¿ï¼š
        1. ç¨³å®šæ€§é«˜ - RSSæ˜¯æ ‡å‡†æ ¼å¼ï¼Œè§£æç®€å•å¯é 
        2. åçˆ¬é£é™©ä½ - ä¸ç›´æ¥è®¿é—®åŸç½‘ç«™HTML
        3. æ•°æ®ç»“æ„åŒ– - XMLæ ¼å¼ï¼Œè§£ææ¯”HTMLç®€å•100å€
        ç‰¹åˆ«é€‚åˆ GitHub Actions ç¯å¢ƒ
        """
        results = []
        self.log("ğŸš€ RSSä¼˜å…ˆæŠ“å–ç­–ç•¥å¯åŠ¨...")
        
        # AIç›¸å…³å…³é”®è¯ï¼Œç”¨äºç­›é€‰éAIä¸“é¢˜RSSä¸­çš„ç›¸å…³å†…å®¹
        ai_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'deep learning',
            'chatgpt', 'gpt', 'llm', 'openai', 'anthropic', 'claude', 'gemini',
            'llama', 'neural', 'transformer', 'model', 'training', 'inference',
            'agent', 'rag', 'embedding', 'diffusion', 'stable diffusion', 'midjourney',
            'copilot', 'generative', 'nlp', 'computer vision', 'robotics',
            'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'æ™ºèƒ½ä½“'
        ]
        
        def is_ai_related(title, description=''):
            """æ£€æŸ¥æ–°é—»æ˜¯å¦ä¸AIç›¸å…³"""
            text = (title + ' ' + description).lower()
            return any(kw in text for kw in ai_keywords)
        
        def parse_rss_item(item, source_name, authority, namespaces=None):
            """è§£æå•ä¸ªRSSæ¡ç›®"""
            # å°è¯•è·å–æ ‡é¢˜
            title = ''
            title_elem = item.find('title')
            if title_elem is not None and title_elem.text:
                title = title_elem.text.strip()
                # å¤„ç†CDATA
                if title.startswith('<![CDATA['):
                    title = title.replace('<![CDATA[', '').replace(']]>', '')
            
            if not title or len(title) < 10:
                return None
            
            # å°è¯•è·å–é“¾æ¥
            link = ''
            link_elem = item.find('link')
            if link_elem is not None:
                link = link_elem.text.strip() if link_elem.text else ''
                if not link:
                    link = link_elem.get('href', '')
            
            if not link:
                guid_elem = item.find('guid')
                if guid_elem is not None and guid_elem.text:
                    if guid_elem.text.startswith('http'):
                        link = guid_elem.text.strip()
            
            if not link or not link.startswith('http'):
                return None
            
            # å°è¯•è·å–æè¿°/å†…å®¹
            description = ''
            for desc_tag in ['description', 'summary', 'content']:
                desc_elem = item.find(desc_tag)
                if desc_elem is not None and desc_elem.text:
                    description = desc_elem.text.strip()
                    # ç§»é™¤HTMLæ ‡ç­¾
                    description = re.sub(r'<[^>]+>', '', description)
                    description = description[:300]
                    break
            
            # å°è¯•è·å–å‘å¸ƒæ—¶é—´
            pub_date = self.yesterday
            for date_tag in ['pubDate', 'published', 'updated', 'dc:date']:
                date_elem = item.find(date_tag)
                if date_elem is not None and date_elem.text:
                    pub_date = date_elem.text.strip()[:10]
                    break
            
            return {
                'title': title,
                'url': link,
                'content': description,
                'publish_time': pub_date,
                'source': source_name,
                'authority_score': authority
            }
        
        # å¹¶è¡Œè·å–æ‰€æœ‰RSSæº
        def fetch_single_rss(name, config):
            """è·å–å•ä¸ªRSSæº"""
            feed_results = []
            try:
                response = self.session.get(config['url'], timeout=15)
                if response.status_code != 200:
                    return feed_results
                
                # è§£æXML
                try:
                    # å°è¯•ç”¨ElementTreeè§£æ
                    root = ET.fromstring(response.content)
                    
                    # æŸ¥æ‰¾æ‰€æœ‰itemæˆ–entryï¼ˆæ”¯æŒRSSå’ŒAtomæ ¼å¼ï¼‰
                    items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
                    
                    for item in items[:15]:  # æ¯æºæœ€å¤š15æ¡
                        parsed = parse_rss_item(item, name, config['authority'])
                        if parsed:
                            # å¯¹äºéAIä¸“é¢˜æºï¼Œéœ€è¦ç­›é€‰AIç›¸å…³å†…å®¹
                            if config['category'] in ['ai_media', 'ai_company', 'academic']:
                                feed_results.append(parsed)
                            elif is_ai_related(parsed['title'], parsed['content']):
                                feed_results.append(parsed)
                                
                except ET.ParseError:
                    # ElementTreeè§£æå¤±è´¥ï¼Œä½¿ç”¨BeautifulSoup
                    soup = BeautifulSoup(response.content, 'html.parser')
                    items = soup.find_all('item') or soup.find_all('entry')
                    
                    for item in items[:15]:
                        title_elem = item.find('title')
                        link_elem = item.find('link')
                        desc_elem = item.find('description') or item.find('summary')
                        
                        title = title_elem.get_text(strip=True) if title_elem else ''
                        link = ''
                        if link_elem:
                            link = link_elem.get_text(strip=True) or link_elem.get('href', '')
                        desc = desc_elem.get_text(strip=True)[:300] if desc_elem else ''
                        
                        if title and link and len(title) >= 10:
                            if config['category'] in ['ai_media', 'ai_company', 'academic']:
                                feed_results.append({
                                    'title': title,
                                    'url': link,
                                    'content': desc,
                                    'publish_time': self.yesterday,
                                    'source': name,
                                    'authority_score': config['authority']
                                })
                            elif is_ai_related(title, desc):
                                feed_results.append({
                                    'title': title,
                                    'url': link,
                                    'content': desc,
                                    'publish_time': self.yesterday,
                                    'source': name,
                                    'authority_score': config['authority']
                                })
                
            except Exception as e:
                self.log(f"RSSæº {name} è·å–å¤±è´¥: {e}", 'WARNING')
            
            return feed_results
        
        # å¹¶è¡Œè·å–æ‰€æœ‰RSSæº
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {
                executor.submit(fetch_single_rss, name, config): name 
                for name, config in self.rss_feeds.items()
            }
            
            for future in as_completed(futures):
                source_name = futures[future]
                try:
                    feed_results = future.result()
                    if feed_results:
                        results.extend(feed_results)
                        self.log(f"  âœ“ {source_name}: {len(feed_results)} æ¡")
                except Exception as e:
                    self.log(f"  âœ— {source_name}: å¤±è´¥ - {e}", 'WARNING')
        
        self.log(f"ğŸ“° RSSæŠ“å–å®Œæˆï¼Œå…±è·å– {len(results)} æ¡æ–°é—»")
        return results
    
    def google_translate(self, text, max_retries=3):
        """
        ä½¿ç”¨Googleç¿»è¯‘APIè¿›è¡Œç¿»è¯‘ï¼ˆå…è´¹æ–¹æ¡ˆï¼‰
        ä½¿ç”¨deep-translatoråº“è°ƒç”¨Googleç¿»è¯‘
        """
        if not text or not text.strip():
            return text
        
        # å¦‚æœæ–‡æœ¬å·²ç»ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(re.sub(r'\s', '', text))
        if total_chars > 0 and chinese_chars / total_chars > 0.6:
            return text
        
        if not TRANSLATOR_AVAILABLE:
            # å›é€€åˆ°è¯å…¸ç¿»è¯‘
            return self.dict_translate(text)
        
        # ä½¿ç”¨Googleç¿»è¯‘API
        for attempt in range(max_retries):
            try:
                translator = GoogleTranslator(source='auto', target='zh-CN')
                translated = translator.translate(text)
                
                if translated:
                    return translated
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    self.log(f"Googleç¿»è¯‘å¤±è´¥ï¼Œå›é€€åˆ°è¯å…¸ç¿»è¯‘: {e}", 'WARNING')
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå›é€€åˆ°è¯å…¸ç¿»è¯‘
        return self.dict_translate(text)
    
    def dict_translate(self, text):
        """è¯å…¸ç¿»è¯‘ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if not text:
            return text
        
        result = text
        
        # åˆå¹¶ä¸¤ä¸ªè¯å…¸
        combined_dict = {**self.translate_map, **self.full_translate_dict}
        
        # æŒ‰ç…§è¯ç»„é•¿åº¦é™åºæ’åˆ—ï¼Œç¡®ä¿é•¿è¯ç»„ä¼˜å…ˆåŒ¹é…
        sorted_items = sorted(combined_dict.items(), key=lambda x: len(x[0]), reverse=True)
        
        for eng, chn in sorted_items:
            if not eng:  # è·³è¿‡ç©ºé”®
                continue
            # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…
            pattern = r'\b' + re.escape(eng) + r'\b'
            result = re.sub(pattern, chn, result, flags=re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹
        result = re.sub(r'\s+', ' ', result).strip()
        result = re.sub(r'\s*([,ï¼Œ.ã€‚:ï¼š;ï¼›!ï¼?ï¼Ÿ])\s*', r'\1', result)
        
        return result
    
    def simple_translate(self, text):
        """ç®€å•çš„å…³é”®è¯ç¿»è¯‘ï¼ˆå¢å¼ºç‰ˆï¼‰- ä¿ç•™ç”¨äºå†…å®¹æ‘˜è¦"""
        if not text:
            return text
        
        result = text
        # æŒ‰ç…§è¯ç»„é•¿åº¦é™åºæ’åˆ—ï¼Œç¡®ä¿é•¿è¯ç»„ä¼˜å…ˆåŒ¹é…
        sorted_items = sorted(self.translate_map.items(), key=lambda x: len(x[0]), reverse=True)
        for eng, chn in sorted_items:
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…
            pattern = r'\b' + re.escape(eng) + r'\b'
            result = re.sub(pattern, chn, result, flags=re.IGNORECASE)
        
        return result
    
    def translate_title(self, title):
        """
        ç¿»è¯‘æ ‡é¢˜ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨Googleç¿»è¯‘APIï¼‰
        """
        if not title or not title.strip():
            return title
        
        # å¦‚æœæ–‡æœ¬å·²ç»ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', title))
        total_chars = len(re.sub(r'\s', '', title))
        if total_chars > 0 and chinese_chars / total_chars > 0.6:
            return title
        
        # ä½¿ç”¨Googleç¿»è¯‘
        return self.google_translate(title)
    
    def is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url:
            return False
        url = url.strip()
        return url.startswith('http://') or url.startswith('https://') and len(url) > 20
    
    def clean_content(self, content):
        """æ¸…ç†å†…å®¹ï¼Œç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦"""
        if not content:
            return ""
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
        content = re.sub(r'[^\w\s\u4e00-\u9fff.,!?\'"-]', '', content)
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content).strip()
        return content[:300]  # é™åˆ¶é•¿åº¦
    
    def search_news_from_web(self, queries):
        """ä»æœç´¢å¼•æ“è·å–æ–°é—»ï¼ˆå¢å¼ºç‰ˆï¼šä¼˜å…ˆä½¿ç”¨MCPå·¥å…·ï¼Œå›é€€åˆ°ç›´æ¥æŠ“å–ï¼‰"""
        
        # === ä¼˜å…ˆä½¿ç”¨MCPå·¥å…·ï¼ˆå‚è€ƒé¡¹ç›®1ï¼‰ ===
        if MCP_AVAILABLE and batch_web_search:
            try:
                self.log("ä½¿ç”¨MCP batch_web_searchå·¥å…·è·å–æ–°é—»...")
                search_tasks = []
                for query in queries:
                    search_tasks.append({
                        'query': f"{query} {self.yesterday}",
                        'num_results': 10,
                        'data_range': 'd',
                        'cursor': 1
                    })
                
                results = batch_web_search(
                    queries=search_tasks,
                    display_text=f"æœç´¢{self.yesterday}çš„AIæ–°é—»",
                    search_type='news'
                )
                
                if results:
                    processed_results = []
                    for item in results:
                        if isinstance(item, dict) and 'title' in item:
                            processed_results.append({
                                'title': item.get('title', ''),
                                'url': item.get('url', item.get('link', '')),
                                'content': item.get('snippet', item.get('description', '')),
                                'publish_time': item.get('date', self.yesterday),
                                'source': item.get('source', 'Web Search')
                            })
                    
                    if processed_results:
                        self.log(f"MCPå·¥å…·è·å– {len(processed_results)} æ¡æ–°é—»")
                        return processed_results
                        
            except Exception as e:
                self.log(f"MCPå·¥å…·æœç´¢å‡ºé”™ï¼Œå›é€€åˆ°ç›´æ¥æŠ“å–: {e}", 'WARNING')
        
        # === å›é€€æ–¹æ¡ˆï¼šç›´æ¥æŠ“å–AIæ–°é—»æº ===
        try:
            results = []
            
            # æ‰©å±•çš„AIæ–°é—»æºåˆ—è¡¨
            ai_news_sources = [
                {
                    'name': 'TechCrunch AI',
                    'url': 'https://techcrunch.com/category/artificial-intelligence/',
                    'selector': 'article',
                    'title_tag': 'h2',
                    'title_class': 'loop-card__title',
                    'link_selector': 'a.loop-card__link',
                    'content_selector': '.loop-card__summary'
                },
                {
                    'name': 'The Verge AI',
                    'url': 'https://www.theverge.com/ai-artificial-intelligence',
                    'selector': 'div',
                    'title_class': 'font-bold',
                    'link_selector': 'a[href*="/2026/"]',
                    'content_selector': ''
                },
                {
                    'name': 'Wired AI',
                    'url': 'https://www.wired.com/tag/artificial-intelligence/',
                    'selector': 'div',
                    'title_tag': 'h3',
                    'link_selector': 'a.summary-item__link',
                    'content_selector': '.summary-item__dek'
                },
                # æ–°å¢æ›´å¤šæ•°æ®æº
                {
                    'name': 'Ars Technica AI',
                    'url': 'https://arstechnica.com/ai/',
                    'selector': 'article',
                    'title_tag': 'h2',
                    'link_selector': 'a[href*="/2026/"]',
                    'content_selector': 'p.excerpt'
                },
                {
                    'name': 'VentureBeat AI',
                    'url': 'https://venturebeat.com/category/ai/',
                    'selector': 'article',
                    'title_tag': 'h2',
                    'link_selector': 'a.article-title',
                    'content_selector': '.article-excerpt'
                }
            ]
            
            for source in ai_news_sources:
                try:
                    response = self.session.get(source['url'], timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
                        links = soup.select(source['link_selector'])
                        seen_urls = set()
                        
                        for link_elem in links[:8]:  # æ¯ç«™æœ€å¤š8æ¡
                            url = link_elem.get('href', '').strip()
                            
                            if not url:
                                continue
                            
                            # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                            if not url.startswith('http'):
                                from urllib.parse import urljoin
                                url = urljoin(source['url'], url)
                            
                            # å»é‡
                            if url in seen_urls:
                                continue
                            seen_urls.add(url)
                            
                            # è·å–æ ‡é¢˜
                            title = ""
                            if source.get('title_tag') and source.get('title_class'):
                                # å°è¯•é€šè¿‡çˆ¶å…ƒç´ æŸ¥æ‰¾
                                parent = link_elem.find_parent()
                                if parent:
                                    title_elem = parent.find(source['title_tag'], class_=source.get('title_class').replace(' ', '.'))
                                    if title_elem:
                                        title = title_elem.get_text(strip=True)
                            
                            if not title:
                                title = link_elem.get_text(strip=True)
                            
                            if not title or len(title) < 10:
                                continue
                            
                            results.append({
                                'title': title,
                                'url': url,
                                'content': '',
                                'publish_time': self.yesterday,
                                'source': source['name']
                            })
                            
                except Exception as e:
                    self.log(f"æŠ“å– {source['name']} å‡ºé”™: {e}", 'WARNING')
                    continue
            
            # å¦‚æœç›´æ¥æŠ“å–å¤±è´¥ï¼Œå›é€€ä½¿ç”¨Bing RSS
            if len(results) < 5:
                self.log("ç›´æ¥æŠ“å–ç»“æœä¸è¶³ï¼Œå°è¯•Bing RSS...", 'INFO')
                fallback_results = self.search_from_bing_rss(queries)
                results.extend(fallback_results)
            
            self.log(f"ç½‘ç»œæœç´¢è·å– {len(results)} æ¡æ–°é—»")
            return results
            
        except Exception as e:
            self.log(f"ç½‘ç»œæœç´¢å‡ºé”™: {e}", 'ERROR')
            return []
    
    def search_from_twitter(self, keywords):
        """ä»Twitter/Xè·å–AIç›¸å…³æ–°é—»ï¼ˆå¢å¼ºç‰ˆï¼šMCPä¼˜å…ˆï¼Œå›é€€åˆ°Nitteré•œåƒï¼‰"""
        
        # === æ–¹æ¡ˆ1ï¼šä¼˜å…ˆä½¿ç”¨MCPå·¥å…·ï¼ˆå‚è€ƒé¡¹ç›®1ï¼‰ ===
        if MCP_AVAILABLE and twitter_search_tweets:
            try:
                all_tweets = []
                for keyword in keywords[:5]:  # é™åˆ¶æœç´¢æ•°é‡
                    tweets = twitter_search_tweets(
                        query=keyword,
                        start_date=self.yesterday,
                        end_date=self.yesterday,
                        lang='en',
                        limit=20,
                        min_likes=5,
                        display_text=f"æœç´¢Twitterä¸Šçš„{self.yesterday}AIæ–°é—»"
                    )
                    if tweets:
                        all_tweets.extend(tweets)
                
                if all_tweets:
                    self.log(f"MCP Twitterå·¥å…·è·å– {len(all_tweets)} æ¡æ¨æ–‡")
                    return all_tweets
            except Exception as e:
                self.log(f"MCP Twitteræœç´¢å‡ºé”™ï¼Œå›é€€åˆ°Nitter: {e}", 'WARNING')
        
        # === æ–¹æ¡ˆ2ï¼šä½¿ç”¨Nitteré•œåƒï¼ˆä¸ä¾èµ–MCPï¼‰ ===
        self.log("ä½¿ç”¨Nitteré•œåƒæŠ“å–Twitteræ•°æ®...")
        return self._search_twitter_via_nitter(keywords)
    
    def _search_twitter_via_nitter(self, keywords):
        """é€šè¿‡Nitteré•œåƒæŠ“å–Twitteræ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        results = []
        
        # Nitteré•œåƒç«™ç‚¹åˆ—è¡¨ï¼ˆå¤šä¸ªå¤‡ç”¨ï¼‰
        nitter_instances = [
            'https://nitter.poast.org',
            'https://nitter.privacydev.net',
            'https://nitter.woodland.cafe',
            'https://nitter.esmailelbob.xyz',
            'https://nitter.1d4.us',
        ]
        
        # AIé¢†åŸŸçŸ¥åè´¦å·ï¼ˆç›´æ¥æŠ“å–å…¶æ—¶é—´çº¿ï¼‰
        ai_accounts = [
            'OpenAI',
            'AnthropicAI', 
            'GoogleAI',
            'DeepMind',
            'nvidia',
            'huaborface',
            'ylecun',
            'kaborepat',
            'sama',
            'DrJimFan',
            'EMostaque',
        ]
        
        # æœç´¢å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        search_queries = [
            'AI%20breakthrough',
            'ChatGPT',
            'Claude%20AI',
            'GPT-4',
            'LLM',
        ]
        
        working_instance = None
        
        # æ‰¾åˆ°ä¸€ä¸ªå¯ç”¨çš„Nitterå®ä¾‹
        for instance in nitter_instances:
            try:
                test_url = f"{instance}/OpenAI"
                response = self.session.get(test_url, timeout=10)
                if response.status_code == 200:
                    working_instance = instance
                    self.log(f"ä½¿ç”¨Nitterå®ä¾‹: {instance}")
                    break
            except Exception:
                continue
        
        if not working_instance:
            self.log("æ‰€æœ‰Nitterå®ä¾‹ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨Twitter RSSå¤‡ç”¨æ–¹æ¡ˆ", 'WARNING')
            return self._search_twitter_via_rss(keywords)
        
        # æ–¹æ³•1ï¼šæŠ“å–AIé¢†åŸŸçŸ¥åè´¦å·çš„æ—¶é—´çº¿
        for account in ai_accounts[:8]:  # é™åˆ¶æ•°é‡
            try:
                url = f"{working_instance}/{account}"
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æŸ¥æ‰¾æ¨æ–‡
                    tweets = soup.select('.timeline-item')
                    
                    for tweet in tweets[:5]:  # æ¯ä¸ªè´¦å·æœ€å¤š5æ¡
                        # æå–æ¨æ–‡å†…å®¹
                        content_elem = tweet.select_one('.tweet-content')
                        if not content_elem:
                            continue
                        
                        text = content_elem.get_text(strip=True)
                        if len(text) < 20:
                            continue
                        
                        # æå–é“¾æ¥
                        link_elem = tweet.select_one('.tweet-link')
                        tweet_url = ""
                        if link_elem:
                            href = link_elem.get('href', '')
                            if href:
                                tweet_url = f"https://twitter.com{href}" if href.startswith('/') else href
                        
                        # æå–æ—¶é—´
                        time_elem = tweet.select_one('.tweet-date a')
                        post_time = self.yesterday
                        if time_elem:
                            time_title = time_elem.get('title', '')
                            if time_title:
                                post_time = time_title
                        
                        # æå–äº’åŠ¨æ•°æ®
                        stats = tweet.select('.tweet-stat')
                        likes = 0
                        retweets = 0
                        for stat in stats:
                            stat_text = stat.get_text(strip=True)
                            if 'like' in stat_text.lower():
                                try:
                                    likes = int(''.join(filter(str.isdigit, stat_text)) or 0)
                                except ValueError:
                                    pass
                            elif 'retweet' in stat_text.lower():
                                try:
                                    retweets = int(''.join(filter(str.isdigit, stat_text)) or 0)
                                except ValueError:
                                    pass
                        
                        results.append({
                            'text': text[:300],
                            'author': {'username': account},
                            'id': tweet_url.split('/')[-1] if tweet_url else '',
                            'posted': post_time,
                            'engagement': {'likes': likes, 'retweets': retweets}
                        })
                        
            except Exception as e:
                self.log(f"æŠ“å– @{account} å‡ºé”™: {e}", 'WARNING')
                continue
        
        # æ–¹æ³•2ï¼šæœç´¢å…³é”®è¯
        for query in search_queries[:3]:
            try:
                url = f"{working_instance}/search?f=tweets&q={query}"
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    tweets = soup.select('.timeline-item')
                    
                    for tweet in tweets[:5]:
                        content_elem = tweet.select_one('.tweet-content')
                        if not content_elem:
                            continue
                        
                        text = content_elem.get_text(strip=True)
                        if len(text) < 20:
                            continue
                        
                        # æå–ç”¨æˆ·å
                        username_elem = tweet.select_one('.username')
                        username = username_elem.get_text(strip=True).replace('@', '') if username_elem else 'unknown'
                        
                        # æå–é“¾æ¥
                        link_elem = tweet.select_one('.tweet-link')
                        tweet_url = ""
                        if link_elem:
                            href = link_elem.get('href', '')
                            if href:
                                tweet_url = f"https://twitter.com{href}" if href.startswith('/') else href
                        
                        results.append({
                            'text': text[:300],
                            'author': {'username': username},
                            'id': tweet_url.split('/')[-1] if tweet_url else '',
                            'posted': self.yesterday,
                            'engagement': {'likes': 0, 'retweets': 0}
                        })
                        
            except Exception as e:
                continue
        
        self.log(f"Nitterè·å– {len(results)} æ¡æ¨æ–‡")
        return results
    
    def _search_twitter_via_rss(self, keywords):
        """é€šè¿‡Twitter RSSå¤‡ç”¨æ–¹æ¡ˆï¼ˆæœ€åå›é€€ï¼‰"""
        results = []
        
        # ä½¿ç”¨ç¬¬ä¸‰æ–¹Twitter RSSæœåŠ¡
        rss_services = [
            # Nitter RSS
            'https://nitter.poast.org/{account}/rss',
            # RSS Bridge
            'https://rss.app/feeds/twitter/{account}.xml',
        ]
        
        ai_accounts = ['OpenAI', 'AnthropicAI', 'GoogleAI', 'DeepMind']
        
        for account in ai_accounts[:4]:
            for rss_template in rss_services:
                try:
                    rss_url = rss_template.format(account=account)
                    response = self.session.get(rss_url, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        items = soup.find_all('item')
                        
                        for item in items[:3]:
                            title_elem = item.find('title')
                            link_elem = item.find('link')
                            desc_elem = item.find('description')
                            
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            if not title or len(title) < 10:
                                continue
                            
                            link = ""
                            if link_elem:
                                link = link_elem.get_text(strip=True) or link_elem.get('href', '')
                            
                            results.append({
                                'text': title[:300],
                                'author': {'username': account},
                                'id': link.split('/')[-1] if link else '',
                                'posted': self.yesterday,
                                'engagement': {'likes': 0, 'retweets': 0}
                            })
                        
                        break  # æˆåŠŸè·å–ï¼Œè·³è¿‡å…¶ä»–RSSæœåŠ¡
                        
                except Exception:
                    continue
        
        self.log(f"RSSå¤‡ç”¨æ–¹æ¡ˆè·å– {len(results)} æ¡æ¨æ–‡")
        return results
    
    def search_from_reddit(self):
        """ä»Redditè·å–AIç›¸å…³æ–°é—»ï¼ˆå¢å¼ºç‰ˆï¼šæ·»åŠ å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        results = []
        
        # Reddit AIç›¸å…³å­ç‰ˆå—
        subreddits = [
            'artificial',
            'MachineLearning',
            'ChatGPT',
            'OpenAI',
            'LocalLLaMA',
            'singularity'
        ]
        
        # æ–¹æ¡ˆ1ï¼šç›´æ¥è®¿é—®Reddit JSON API
        for subreddit in subreddits:
            try:
                # ä½¿ç”¨Redditçš„JSON APIï¼ˆæ— éœ€è®¤è¯ï¼‰
                url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
                headers = {
                    'User-Agent': 'AI News Collector Bot 1.0'
                }
                
                response = self.session.get(url, headers=headers, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    posts = data.get('data', {}).get('children', [])
                    
                    for post in posts:
                        post_data = post.get('data', {})
                        
                        # è¿‡æ»¤æ¡ä»¶ï¼šè¯„åˆ†>50ï¼Œä¸”æ˜¯è¿‘æœŸå¸–å­
                        score = post_data.get('score', 0)
                        if score < 50:
                            continue
                        
                        title = post_data.get('title', '')
                        permalink = post_data.get('permalink', '')
                        selftext = post_data.get('selftext', '')[:300]
                        created_utc = post_data.get('created_utc', 0)
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿‘æœŸå¸–å­ï¼ˆ24å°æ—¶å†…ï¼‰
                        if created_utc:
                            post_time = datetime.fromtimestamp(created_utc)
                            if (datetime.now() - post_time).days > 2:
                                continue
                        
                        if title and permalink:
                            results.append({
                                'title': title,
                                'url': f"https://www.reddit.com{permalink}",
                                'content': selftext,
                                'publish_time': self.yesterday,
                                'source': f"Reddit r/{subreddit}",
                                'engagement': {'likes': score}
                            })
                            
            except Exception as e:
                self.log(f"Reddit r/{subreddit} æŠ“å–å‡ºé”™: {e}", 'WARNING')
                continue
        
        # æ–¹æ¡ˆ2ï¼šå¦‚æœç›´æ¥è®¿é—®å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Reddit RSS
        if len(results) == 0:
            self.log("Reddit APIä¸å¯ç”¨ï¼Œå°è¯•RSSå¤‡ç”¨æ–¹æ¡ˆ...", 'INFO')
            results = self._search_reddit_via_rss(subreddits)
        
        self.log(f"Redditè·å– {len(results)} æ¡å¸–å­")
        return results
    
    def _search_reddit_via_rss(self, subreddits):
        """é€šè¿‡RSSè·å–Redditå†…å®¹ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        results = []
        
        for subreddit in subreddits[:4]:
            try:
                # Reddit RSS
                rss_url = f"https://www.reddit.com/r/{subreddit}/hot.rss"
                response = self.session.get(rss_url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    entries = soup.find_all('entry')
                    
                    for entry in entries[:5]:
                        title_elem = entry.find('title')
                        link_elem = entry.find('link')
                        content_elem = entry.find('content')
                        
                        title = title_elem.get_text(strip=True) if title_elem else ""
                        if not title or len(title) < 10:
                            continue
                        
                        link = link_elem.get('href', '') if link_elem else ""
                        content = ""
                        if content_elem:
                            content = self.clean_content(content_elem.get_text(strip=True))
                        
                        results.append({
                            'title': title,
                            'url': link,
                            'content': content[:200],
                            'publish_time': self.yesterday,
                            'source': f"Reddit r/{subreddit}",
                            'engagement': {'likes': 0}
                        })
                        
            except Exception:
                continue
        
        return results
    
    def search_from_bing_rss(self, queries):
        """ä»Bing RSSè·å–æ–°é—»ï¼ˆä¼˜åŒ–ç‰ˆï¼šè·å–çœŸå®URLï¼‰"""
        results = []
        
        for query in queries[:3]:
            search_url = "https://www.bing.com/news/search"
            params = {
                'q': f'{query} {self.yesterday}',
                'format': 'rss',
                'count': 10
            }
            
            try:
                response = self.session.get(search_url, params=params, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    items = soup.find_all('item')
                    
                    for item in items[:5]:
                        title_elem = item.find('title')
                        link_elem = item.find('link')
                        
                        title = title_elem.get_text(strip=True) if title_elem else ""
                        
                        if not title:
                            continue
                        
                        # å°è¯•å¤šç§æ–¹å¼è·å–çœŸå®URL
                        url = ""
                        
                        # æ–¹å¼1ï¼šç›´æ¥è·å–linkæ ‡ç­¾
                        if link_elem:
                            url = link_elem.get_text(strip=True) or link_elem.get('href', '')
                        
                        # æ–¹å¼2ï¼šä»news:urlæˆ–entityå…ƒç´ è·å–
                        if not url or len(url) < 20:
                            news_url = item.find('news:url')
                            if news_url:
                                url = news_url.get_text(strip=True)
                        
                        # æ–¹å¼3ï¼šå°è¯•ä»URLæ¨æ–­æˆ–æ„å»ºæœç´¢é“¾æ¥
                        if not url or 'bing.com' in url:
                            # ä½¿ç”¨æœç´¢é“¾æ¥ä½†æ ‡è®°ä¸ºæœç´¢ç»“æœ
                            encoded_query = query.replace(' ', '+')
                            url = f"https://www.bing.com/news/search?q={encoded_query}+{self.yesterday}"
                        
                        # æ¸…ç†å†…å®¹
                        desc_elem = item.find('description')
                        desc = desc_elem.get_text(strip=True) if desc_elem else ""
                        desc = self.clean_content(desc)
                        
                        # è·å–æ¥æº
                        source_elem = item.find('source')
                        source = source_elem.get_text(strip=True) if source_elem else "AI News"
                        
                        results.append({
                            'title': title,
                            'url': url,
                            'content': desc,
                            'publish_time': self.yesterday,
                            'source': source
                        })
                        
            except Exception as e:
                continue
        
        return results
    
    def search_from_hacker_news(self):
        """ä»Hacker Newsè·å–AIç›¸å…³æ–°é—»ï¼ˆä½¿ç”¨å®˜æ–¹APIï¼Œç¨³å®šæ— åçˆ¬ï¼‰"""
        results = []
        
        self.log("æ­£åœ¨ä»Hacker Newsè·å–AIç›¸å…³æ–°é—»...")
        
        # AIç›¸å…³æœç´¢å…³é”®è¯
        ai_keywords = [
            'AI', 'artificial intelligence', 'ChatGPT', 'GPT', 'LLM',
            'OpenAI', 'Anthropic', 'Claude', 'Gemini', 'Llama',
            'machine learning', 'deep learning', 'neural network',
            'AGI', 'transformer', 'diffusion', 'RLHF'
        ]
        
        try:
            # æ–¹æ¡ˆ1ï¼šä½¿ç”¨HN Algolia Search APIï¼ˆæœ€ç¨³å®šï¼Œæ”¯æŒæœç´¢ï¼‰
            # è·å–æœ€è¿‘24å°æ—¶çš„AIç›¸å…³å¸–å­
            yesterday_ts = int((datetime.now() - timedelta(days=1)).timestamp())
            
            for keyword in ai_keywords[:5]:  # é™åˆ¶æœç´¢æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚
                try:
                    search_url = "https://hn.algolia.com/api/v1/search"
                    params = {
                        'query': keyword,
                        'tags': 'story',  # åªæœç´¢æ•…äº‹ï¼ˆéè¯„è®ºï¼‰
                        'numericFilters': f'created_at_i>{yesterday_ts}',
                        'hitsPerPage': 10
                    }
                    
                    response = self.session.get(search_url, params=params, timeout=10)
                    if response.status_code == 200:
                        data = response.json()
                        hits = data.get('hits', [])
                        
                        for hit in hits:
                            title = hit.get('title', '')
                            if not title or len(title) < 10:
                                continue
                            
                            # è·å–URLï¼ˆä¼˜å…ˆä½¿ç”¨åŸå§‹é“¾æ¥ï¼Œå¦åˆ™ä½¿ç”¨HNè®¨è®ºé¡µï¼‰
                            url = hit.get('url', '')
                            story_id = hit.get('objectID', '')
                            if not url:
                                url = f"https://news.ycombinator.com/item?id={story_id}"
                            
                            # è·å–äº’åŠ¨æ•°æ®
                            points = hit.get('points', 0)
                            num_comments = hit.get('num_comments', 0)
                            
                            results.append({
                                'title': title,
                                'url': url,
                                'content': '',
                                'publish_time': self.yesterday,
                                'source': 'Hacker News',
                                'engagement': {
                                    'likes': points,
                                    'comments': num_comments
                                },
                                'hn_discussion': f"https://news.ycombinator.com/item?id={story_id}"
                            })
                            
                    time.sleep(0.2)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    self.log(f"HNæœç´¢å…³é”®è¯ '{keyword}' å‡ºé”™: {e}", 'WARNING')
                    continue
            
            # æ–¹æ¡ˆ2ï¼šè·å–HNé¦–é¡µçƒ­é—¨å¸–å­ï¼Œç­›é€‰AIç›¸å…³
            try:
                # è·å–Top Stories
                top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
                response = self.session.get(top_url, timeout=10)
                
                if response.status_code == 200:
                    story_ids = response.json()[:50]  # å–å‰50ä¸ªçƒ­é—¨å¸–å­
                    
                    for story_id in story_ids[:30]:  # é™åˆ¶è¯·æ±‚æ•°é‡
                        try:
                            item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                            item_response = self.session.get(item_url, timeout=5)
                            
                            if item_response.status_code == 200:
                                item = item_response.json()
                                if not item:
                                    continue
                                
                                title = item.get('title', '').lower()
                                
                                # æ£€æŸ¥æ˜¯å¦ä¸AIç›¸å…³
                                is_ai_related = any(kw.lower() in title for kw in ai_keywords)
                                if not is_ai_related:
                                    continue
                                
                                original_title = item.get('title', '')
                                url = item.get('url', '')
                                if not url:
                                    url = f"https://news.ycombinator.com/item?id={story_id}"
                                
                                points = item.get('score', 0)
                                num_comments = item.get('descendants', 0)
                                
                                # é¿å…é‡å¤
                                if any(r['url'] == url for r in results):
                                    continue
                                
                                results.append({
                                    'title': original_title,
                                    'url': url,
                                    'content': '',
                                    'publish_time': self.yesterday,
                                    'source': 'Hacker News',
                                    'engagement': {
                                        'likes': points,
                                        'comments': num_comments
                                    },
                                    'hn_discussion': f"https://news.ycombinator.com/item?id={story_id}"
                                })
                                
                        except Exception:
                            continue
                        
                        time.sleep(0.1)  # APIé™æµ
                        
            except Exception as e:
                self.log(f"HN Top Storiesè·å–å‡ºé”™: {e}", 'WARNING')
        
        except Exception as e:
            self.log(f"Hacker NewsæŠ“å–å‡ºé”™: {e}", 'ERROR')
        
        # æŒ‰ç‚¹èµæ•°æ’åºï¼Œå–å‰20æ¡
        results = sorted(results, key=lambda x: x.get('engagement', {}).get('likes', 0), reverse=True)[:20]
        
        self.log(f"Hacker Newsè·å– {len(results)} æ¡AIç›¸å…³æ–°é—»")
        return results
    
    def search_from_meta_ai_blog(self):
        """ä»Meta AI (FAIR) Blogè·å–æ–°é—»"""
        results = []
        
        self.log("æ­£åœ¨ä»Meta AI Blogè·å–æ–°é—»...")
        
        try:
            url = "https://ai.meta.com/blog/"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                seen_urls = set()
                
                # ä¼˜åŒ–çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼ˆæ ¹æ®é¡µé¢ç»“æ„ï¼‰
                # Meta AI Blogä½¿ç”¨articleæ ‡ç­¾å’Œh2/h3æ ‡é¢˜
                article_selectors = [
                    'article h2 a',
                    'article h3 a',
                    '.blog-post h2 a',
                    '.blog-post h3 a',
                    '.blog-item a',
                    'a[href*="/blog/"][href*="meta"]',
                ]
                
                for selector in article_selectors:
                    links = soup.select(selector)
                    
                    for link in links[:15]:
                        href = link.get('href', '').strip()
                        if not href:
                            continue
                        
                        # æ’é™¤ä¸»é¡µé“¾æ¥
                        if href in ['/', '/blog/', '/blog', 'https://ai.meta.com/blog/']:
                            continue
                        
                        # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                        if not href.startswith('http'):
                            from urllib.parse import urljoin
                            href = urljoin('https://ai.meta.com', href)
                        
                        # ç¡®ä¿æ˜¯åšå®¢æ–‡ç« é“¾æ¥
                        if '/blog/' not in href:
                            continue
                        
                        # å»é‡
                        if href in seen_urls:
                            continue
                        seen_urls.add(href)
                        
                        # è·å–æ ‡é¢˜
                        title = link.get_text(strip=True)
                        if not title:
                            title = link.get('title', '')
                        
                        # æ¸…ç†æ ‡é¢˜
                        title = ' '.join(title.split())
                        
                        if not title or len(title) < 10 or len(title) > 300:
                            continue
                        
                        results.append({
                            'title': title,
                            'url': href,
                            'content': '',
                            'publish_time': self.yesterday,
                            'source': 'Meta AI Blog'
                        })
                    
                    if results:
                        break  # å¦‚æœæ‰¾åˆ°äº†ç»“æœï¼Œä¸å†å°è¯•å…¶ä»–é€‰æ‹©å™¨
                
                # å¤‡ç”¨æ–¹æ¡ˆï¼šéå†æ‰€æœ‰é“¾æ¥æŸ¥æ‰¾åšå®¢æ–‡ç« 
                if not results:
                    all_links = soup.find_all('a', href=True)
                    for link in all_links:
                        href = link.get('href', '')
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åšå®¢æ–‡ç« é“¾æ¥
                        if '/blog/' not in href:
                            continue
                        if href in ['/', '/blog/', '/blog']:
                            continue
                        
                        # è¡¥å…¨é“¾æ¥
                        if not href.startswith('http'):
                            href = f"https://ai.meta.com{href}"
                        
                        # ç¡®ä¿æ˜¯ai.meta.comåŸŸå
                        if 'ai.meta.com' not in href:
                            continue
                        
                        if href in seen_urls:
                            continue
                        seen_urls.add(href)
                        
                        title = link.get_text(strip=True)
                        # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªé•¿çš„æ ‡é¢˜
                        if title and 10 < len(title) < 200:
                            # æ’é™¤å¯¼èˆªç±»æ–‡å­—
                            if title.lower() not in ['blog', 'learn more', 'read more', 'view all']:
                                results.append({
                                    'title': title,
                                    'url': href,
                                    'content': '',
                                    'publish_time': self.yesterday,
                                    'source': 'Meta AI Blog'
                                })
            
            # å¤‡ç”¨æ–¹æ¡ˆ2ï¼šé€šè¿‡Bingæœç´¢è·å–Meta AI Blogæœ€æ–°æ–‡ç« 
            # ï¼ˆMeta AI Blogä½¿ç”¨JavaScriptæ¸²æŸ“ï¼Œç›´æ¥æŠ“å–å¯èƒ½è·å–ä¸åˆ°å†…å®¹ï¼‰
            if len(results) < 3:
                try:
                    search_url = "https://www.bing.com/news/search"
                    params = {
                        'q': 'site:ai.meta.com/blog',
                        'format': 'rss',
                        'count': 10
                    }
                    search_response = self.session.get(search_url, params=params, timeout=10)
                    
                    if search_response.status_code == 200:
                        search_soup = BeautifulSoup(search_response.text, 'html.parser')
                        items = search_soup.find_all('item')
                        
                        for item in items[:5]:
                            title_elem = item.find('title')
                            link_elem = item.find('link')
                            
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            link_href = ""
                            if link_elem:
                                link_href = link_elem.get_text(strip=True)
                            
                            if not title or len(title) < 10:
                                continue
                            
                            # ç¡®ä¿é“¾æ¥æŒ‡å‘ai.meta.com
                            if link_href and 'ai.meta.com' in link_href and link_href not in seen_urls:
                                seen_urls.add(link_href)
                                results.append({
                                    'title': title,
                                    'url': link_href,
                                    'content': '',
                                    'publish_time': self.yesterday,
                                    'source': 'Meta AI Blog'
                                })
                                
                except Exception:
                    pass
                
        except Exception as e:
            self.log(f"Meta AI BlogæŠ“å–å‡ºé”™: {e}", 'ERROR')
        
        self.log(f"Meta AI Blogè·å– {len(results)} æ¡æ–°é—»")
        return results[:10]  # é™åˆ¶æ•°é‡
    
    def search_from_microsoft_research_blog(self):
        """ä»Microsoft Research Blogè·å–AIç›¸å…³æ–°é—»"""
        results = []
        
        self.log("æ­£åœ¨ä»Microsoft Research Blogè·å–æ–°é—»...")
        
        try:
            # Microsoft Research Blogä¸»é¡µ
            url = "https://www.microsoft.com/en-us/research/blog/"
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                seen_urls = set()
                
                # ä¼˜åŒ–é€‰æ‹©å™¨ï¼Œæ’é™¤äººå‘˜é¡µé¢ï¼Œåªè·å–åšå®¢æ–‡ç« 
                # åšå®¢æ–‡ç« URLé€šå¸¸åŒ…å«æ—¥æœŸæˆ–ç‰¹å®šè·¯å¾„æ¨¡å¼
                all_links = soup.find_all('a', href=True)
                
                for link in all_links:
                    href = link.get('href', '').strip()
                    if not href:
                        continue
                    
                    # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                    if not href.startswith('http'):
                        from urllib.parse import urljoin
                        href = urljoin('https://www.microsoft.com', href)
                    
                    # åªæ¥å—åšå®¢æ–‡ç« é“¾æ¥ï¼ˆåŒ…å«/blog/ä¸”ä¸æ˜¯äººå‘˜é¡µé¢ï¼‰
                    # æ’é™¤æ¡ä»¶ï¼š
                    # - äººå‘˜é¡µé¢ (/people/)
                    # - ä¸»é¡µ (/blog/ æœ¬èº«)
                    # - é¡¹ç›®é¡µé¢ (/project/)
                    # - å›¢é˜Ÿé¡µé¢ (/group/)
                    if '/research/blog/' not in href:
                        continue
                    if '/people/' in href or '/project/' in href or '/group/' in href:
                        continue
                    if href.endswith('/blog/') or href.endswith('/blog'):
                        continue
                    
                    # ç¡®ä¿æ˜¯microsoft.comåŸŸå
                    if 'microsoft.com' not in href:
                        continue
                    
                    # å»é‡
                    if href in seen_urls:
                        continue
                    seen_urls.add(href)
                    
                    # è·å–æ ‡é¢˜
                    title = ""
                    # å°è¯•ä»é“¾æ¥å†…çš„æ ‡é¢˜å…ƒç´ è·å–
                    title_elem = link.find(['h2', 'h3', 'h4', 'span'])
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    if not title:
                        title = link.get_text(strip=True)
                    if not title:
                        title = link.get('title', '')
                    
                    # æ¸…ç†æ ‡é¢˜
                    title = ' '.join(title.split())
                    
                    # è¿‡æ»¤æ— æ•ˆæ ‡é¢˜
                    if not title or len(title) < 10 or len(title) > 300:
                        continue
                    # æ’é™¤å¯¼èˆªç±»æ–‡å­—
                    if title.lower() in ['blog', 'read more', 'learn more', 'view all', 'microsoft research blog']:
                        continue
                    
                    results.append({
                        'title': title,
                        'url': href,
                        'content': '',
                        'publish_time': self.yesterday,
                        'source': 'Microsoft Research Blog'
                    })
            
            # å°è¯•è·å–AIä¸“é¢˜é¡µé¢ï¼ˆRSSæˆ–APIï¼‰
            if len(results) < 3:
                try:
                    # å°è¯•RSS Feed
                    rss_url = "https://www.microsoft.com/en-us/research/feed/"
                    rss_response = self.session.get(rss_url, timeout=10)
                    
                    if rss_response.status_code == 200:
                        rss_soup = BeautifulSoup(rss_response.text, 'html.parser')
                        items = rss_soup.find_all('item')
                        
                        for item in items[:10]:
                            title_elem = item.find('title')
                            link_elem = item.find('link')
                            
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            link_href = ""
                            if link_elem:
                                link_href = link_elem.get_text(strip=True) or link_elem.next_sibling
                                if link_href:
                                    link_href = str(link_href).strip()
                            
                            if not title or not link_href or len(title) < 10:
                                continue
                            
                            # æ’é™¤äººå‘˜é¡µé¢
                            if '/people/' in link_href:
                                continue
                            
                            if link_href in seen_urls:
                                continue
                            seen_urls.add(link_href)
                            
                            results.append({
                                'title': title,
                                'url': link_href,
                                'content': '',
                                'publish_time': self.yesterday,
                                'source': 'Microsoft Research'
                            })
                            
                except Exception:
                    pass
                
        except Exception as e:
            self.log(f"Microsoft Research BlogæŠ“å–å‡ºé”™: {e}", 'ERROR')
        
        self.log(f"Microsoft Research Blogè·å– {len(results)} æ¡æ–°é—»")
        return results[:10]  # é™åˆ¶æ•°é‡
    
    def search_from_chinese_sources(self):
        """ä»ä¸­æ–‡AIæ–°é—»æºè·å–å†…å®¹ï¼ˆå¢å¼ºå›½å†…è®¿é—®ç¨³å®šæ€§ï¼‰"""
        results = []
        
        # ä¸­æ–‡AIæ–°é—»æº
        chinese_sources = [
            {
                'name': 'æœºå™¨ä¹‹å¿ƒ',
                'url': 'https://www.jiqizhixin.com/',
                'link_selector': 'a[href*="/article/"]',
                'title_attr': 'title'
            },
            {
                'name': 'é‡å­ä½',
                'url': 'https://www.qbitai.com/',
                'link_selector': 'a.post-title',
                'title_attr': None
            },
            {
                'name': 'æ–°æ™ºå…ƒ',
                'url': 'https://www.ailab.cn/',
                'link_selector': 'a[href*="/article-"]',
                'title_attr': None
            },
            {
                'name': '36æ°ªAI',
                'url': 'https://36kr.com/information/AI/',
                'link_selector': 'a.article-item-title',
                'title_attr': None
            }
        ]
        
        for source in chinese_sources:
            try:
                response = self.session.get(source['url'], timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    links = soup.select(source['link_selector'])
                    
                    seen_urls = set()
                    for link in links[:5]:
                        url = link.get('href', '').strip()
                        if not url:
                            continue
                        
                        # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                        if not url.startswith('http'):
                            from urllib.parse import urljoin
                            url = urljoin(source['url'], url)
                        
                        if url in seen_urls:
                            continue
                        seen_urls.add(url)
                        
                        # è·å–æ ‡é¢˜
                        if source.get('title_attr'):
                            title = link.get(source['title_attr'], '')
                        else:
                            title = link.get_text(strip=True)
                        
                        if not title or len(title) < 5:
                            continue
                        
                        results.append({
                            'title': title,
                            'url': url,
                            'content': '',
                            'publish_time': self.yesterday,
                            'source': source['name']
                        })
                        
            except Exception as e:
                self.log(f"æŠ“å– {source['name']} å‡ºé”™: {e}", 'WARNING')
                continue
        
        self.log(f"ä¸­æ–‡æ–°é—»æºè·å– {len(results)} æ¡æ–°é—»")
        return results
    
    def extract_from_websites(self, urls):
        """ä»æŒ‡å®šç½‘ç«™æå–å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        results = []
        
        for url in urls:
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æå–æ ‡é¢˜
                    title_elem = soup.find('h1')
                    title = title_elem.get_text(strip=True) if title_elem else ""
                    
                    if not title:
                        # å°è¯•ä»titleæ ‡ç­¾è·å–
                        title_tag = soup.find('title')
                        if title_tag:
                            title = title_tag.get_text(strip=True)
                    
                    if not title:
                        continue
                    
                    # æå–ä¸»è¦å†…å®¹
                    content_parts = []
                    for p in soup.find_all('p')[:15]:
                        text = p.get_text(strip=True)
                        if len(text) > 30:  # è¿‡æ»¤çŸ­æ–‡æœ¬
                            content_parts.append(text)
                    
                    content = ' '.join(content_parts)
                    content = self.clean_content(content)
                    
                    # ç¡®ä¿æœ‰URL
                    if not self.is_valid_url(url):
                        continue
                    
                    # æå–æ¥æº
                    source = url.split('/')[2]
                    
                    results.append({
                        'title': title,
                        'source': source,
                        'content': content,
                        'url': url,
                        'publish_time': self.yesterday
                    })
                    
            except Exception as e:
                self.log(f"æå– {url} å‡ºé”™: {e}", 'WARNING')
                continue
        
        self.log(f"ç½‘ç«™æå–è·å– {len(results)} æ¡æœ‰æ•ˆæ–°é—»")
        return results
    
    def calculate_score(self, news_item):
        """
        è®¡ç®—æ–°é—»ç»¼åˆè¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒç¤¾äº¤åª’ä½“æ¥æºï¼‰
        è¯„åˆ†ç»´åº¦ï¼š
        - é‡è¦æ€§: 35% (é‡å¤§çªç ´ã€äº§å“å‘å¸ƒã€æ”¿ç­–å˜åŒ–ç­‰)
        - æƒå¨æ€§: 25% (æ¥æºå¯ä¿¡åº¦)
        - ä¼ æ’­åº¦: 20% (ç¤¾äº¤æŒ‡æ ‡)
        - åˆ›æ–°æ€§: 10% (æŠ€æœ¯åˆ›æ–°)
        - æ—¶æ•ˆæ€§: 10% (å‘å¸ƒæ—¶é—´)
        """
        importance = 3  # åŸºç¡€åˆ†é™ä½
        authority = 5
        spread = 5
        innovation = 3
        timeliness = 5
        
        title = news_item.get('title', '').lower()
        source = news_item.get('source', '').lower()
        
        # === é‡è¦æ€§è¯„ä¼° (35%æƒé‡) ===
        # é‡å¤§äº‹ä»¶å…³é”®è¯
        major_keywords = [
            'breakthrough', 'major', 'significant', 'warning', 'alert',
            'critical', 'emergency', 'crisis', 'major update', 'major release'
        ]
        # å‘å¸ƒç±»å…³é”®è¯
        launch_keywords = [
            'launch', 'release', 'announce', 'unveil', 'debut', 'introduce',
            'launches', 'releases', 'announces', 'unveils', 'new model'
        ]
        # å•†ä¸šç±»å…³é”®è¯
        business_keywords = [
            'acquire', 'acquisition', 'partnership', 'collaboration', 'invest',
            'funding', 'ipo', 'public', 'deal', 'merge'
        ]
        # æ”¿ç­–ç±»å…³é”®è¯
        policy_keywords = [
            'regulation', 'policy', 'law', 'ban', 'restriction', 'government',
            'congress', 'parliament', 'EU', 'China', 'US'
        ]
        
        importance_score = 0
        for keyword in major_keywords:
            if keyword in title:
                importance_score += 2
                break
        
        for keyword in launch_keywords:
            if keyword in title:
                importance_score += 3
                break
        
        for keyword in business_keywords:
            if keyword in title:
                importance_score += 2
                break
        
        for keyword in policy_keywords:
            if keyword in title:
                importance_score += 3
                break
        
        # é™åˆ¶é‡è¦æ€§åˆ†æ•°èŒƒå›´
        importance = min(10, max(3, 3 + importance_score))
        
        # === æƒå¨æ€§è¯„ä¼° (25%æƒé‡) ===
        # ä¸€çº§æƒå¨æ¥æºï¼ˆAIå…¬å¸å®˜æ–¹ã€é¡¶çº§åª’ä½“ï¼‰
        top_authoritative = [
            'openai.com', 'anthropic.com', 'deepmind.google', 'ai.googleblog.com',
            'blog.google', 'ai.google', 'ai.meta.com', 'microsoft.com/en-us/research',
            'reuters.com', 'bloomberg.com', 'wsj.com', 'nytimes.com'
        ]
        # äºŒçº§æƒå¨æ¥æºï¼ˆç§‘æŠ€åª’ä½“ã€æŠ€æœ¯ç¤¾åŒºï¼‰
        second_authoritative = [
            'techcrunch.com', 'wired.com', 'theverge.com', 'arstechnica.com',
            'mit.edu', 'stanford.edu', 'google.com', 'meta.com', 'microsoft.com',
            'amazon.com', 'apple.com', 'nvidia.com', 'venturebeat.com',
            'news.ycombinator.com', 'hn.algolia.com'  # Hacker News
        ]
        # ç¤¾äº¤åª’ä½“æ¥æºï¼ˆTwitterã€Redditï¼‰
        social_sources = ['twitter', 'reddit']
        
        authority = 5  # åŸºç¡€åˆ†
        for src in top_authoritative:
            if src in source:
                authority = 9
                break
        if authority == 5:
            for src in second_authoritative:
                if src in source:
                    authority = 7
                    break
        if authority == 5:
            for src in social_sources:
                if src in source:
                    authority = 6  # ç¤¾äº¤åª’ä½“åŸºç¡€åˆ†ç¨ä½
                    break
        
        # === ä¼ æ’­åº¦è¯„ä¼° (20%æƒé‡) - æ–°å¢ ===
        spread = 5  # åŸºç¡€åˆ†
        engagement = news_item.get('engagement', {})
        if isinstance(engagement, dict):
            likes = engagement.get('likes', 0)
            if likes > 1000:
                spread = 9
            elif likes > 500:
                spread = 8
            elif likes > 100:
                spread = 7
            elif likes > 50:
                spread = 6
        
        # === åˆ›æ–°æ€§è¯„ä¼° (10%æƒé‡) ===
        innovation_keywords = [
            'new', 'first', 'innovative', 'revolutionary', 'novel',
            'open source', 'framework', 'architecture', 'prototype',
            'gpt-5', 'gpt-4.5', 'claude 4', 'llama 4', 'mistral large'
        ]
        
        innovation = 5
        for keyword in innovation_keywords:
            if keyword in title:
                innovation += 2
                break
        innovation = min(10, innovation)
        
        # === æ—¶æ•ˆæ€§è¯„ä¼° (10%æƒé‡) ===
        # å·²ç»æ˜¯å‰ä¸€å¤©çš„æ–°é—»
        timeliness = 6  # åŸºç¡€åˆ†
        publish_time = news_item.get('publish_time', '')
        if self.yesterday in publish_time:
            timeliness = 8  # ç²¾ç¡®åŒ¹é…å‰ä¸€å¤©
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆè°ƒæ•´æƒé‡ï¼‰
        total_score = (
            importance * 0.35 +  # é‡è¦æ€§35%
            authority * 0.25 +   # æƒå¨æ€§25%
            spread * 0.20 +      # ä¼ æ’­åº¦20%
            innovation * 0.10 +  # åˆ›æ–°æ€§10%
            timeliness * 0.10    # æ—¶æ•ˆæ€§10%
        )
        
        return {
            'importance': importance,
            'authority': authority,
            'spread': spread,
            'innovation': innovation,
            'timeliness': timeliness,
            'total_score': round(total_score, 2)
        }
    
    def translate_news(self, news_item):
        """ç¿»è¯‘æ–°é—»ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨Googleç¿»è¯‘APIï¼‰"""
        # ç¿»è¯‘æ ‡é¢˜ï¼ˆä½¿ç”¨Googleç¿»è¯‘ï¼Œç¡®ä¿æ ‡é¢˜ä¸ºä¸­æ–‡ï¼‰
        title = news_item.get('title', '')
        translated_title = self.translate_title(title)
        
        # ç¿»è¯‘å†…å®¹æ‘˜è¦ï¼ˆåŒæ ·ä½¿ç”¨Googleç¿»è¯‘APIè¿›è¡Œå®Œæ•´æ±‰åŒ–ï¼‰
        content = news_item.get('content', '')
        translated_content = self.translate_content(content) if content else ''
        
        # æ¥æºä¿æŒåŸæ ·
        source = news_item.get('source', '')
        
        return {
            **news_item,
            'title': translated_title,
            'content': translated_content,
            'source': source,
            'original_title': title,  # ä¿å­˜åŸæ–‡æ ‡é¢˜
            'original_content': content  # ä¿å­˜åŸæ–‡å†…å®¹
        }
    
    def translate_content(self, content):
        """
        ç¿»è¯‘å†…å®¹æ‘˜è¦ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨Googleç¿»è¯‘APIï¼‰
        å¯¹é•¿æ–‡æœ¬è¿›è¡Œåˆ†æ®µç¿»è¯‘ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
        """
        if not content or not content.strip():
            return content
        
        # å¦‚æœæ–‡æœ¬å·²ç»ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        total_chars = len(re.sub(r'\s', '', content))
        if total_chars > 0 and chinese_chars / total_chars > 0.6:
            return content
        
        # å¯¹äºè¾ƒçŸ­çš„å†…å®¹ï¼Œç›´æ¥ç¿»è¯‘
        if len(content) <= 500:
            return self.google_translate(content)
        
        # å¯¹äºè¾ƒé•¿çš„å†…å®¹ï¼ŒæŒ‰å¥å­åˆ†æ®µç¿»è¯‘ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = re.split(r'(?<=[.!?])\s+', content)
        
        translated_parts = []
        current_batch = ""
        
        for sentence in sentences:
            # å¦‚æœå½“å‰æ‰¹æ¬¡åŠ ä¸Šæ–°å¥å­ä¸è¶…è¿‡500å­—ç¬¦ï¼Œåˆ™åˆå¹¶
            if len(current_batch) + len(sentence) <= 500:
                current_batch += (" " if current_batch else "") + sentence
            else:
                # ç¿»è¯‘å½“å‰æ‰¹æ¬¡
                if current_batch:
                    translated_batch = self.google_translate(current_batch)
                    translated_parts.append(translated_batch)
                current_batch = sentence
        
        # ç¿»è¯‘æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            translated_batch = self.google_translate(current_batch)
            translated_parts.append(translated_batch)
        
        # åˆå¹¶æ‰€æœ‰ç¿»è¯‘ç»“æœ
        return " ".join(translated_parts)
    
    def collect_all_news(self):
        """
        æ”¶é›†æ‰€æœ‰å¹³å°çš„AIæ–°é—»ï¼ˆå¢å¼ºç‰ˆ - RSSä¼˜å…ˆç­–ç•¥ï¼‰
        ç­–ç•¥ä¼˜å…ˆçº§ï¼š
        1. RSS Feedsï¼ˆæœ€ç¨³å®šï¼Œåçˆ¬é£é™©æœ€ä½ï¼Œç‰¹åˆ«é€‚åˆGitHub Actionsï¼‰
        2. å®˜æ–¹APIï¼ˆå¦‚Hacker News APIï¼‰
        3. ç½‘é¡µæŠ“å–ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        """
        self.log(f"å¼€å§‹æ”¶é›† {self.yesterday} çš„AIæ–°é—»...")
        
        # ============== ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šRSS Feeds ==============
        # RSSæ˜¯æœ€ç¨³å®šçš„æ•°æ®æºï¼Œè§£æç®€å•ï¼Œåçˆ¬é£é™©ä½
        rss_results = self.fetch_rss_feeds()
        
        # å¤„ç†RSSç»“æœ
        for item in rss_results:
            if isinstance(item, dict) and 'title' in item and 'url' in item:
                if self.is_valid_url(item['url']):
                    # RSSå·²æœ‰authority_scoreï¼Œç›´æ¥ä½¿ç”¨
                    auth_score = item.pop('authority_score', 7)
                    score = self.calculate_score(item)
                    # ä½¿ç”¨RSSæä¾›çš„æƒå¨æ€§åˆ†æ•°
                    score['authority'] = auth_score
                    # é‡æ–°è®¡ç®—æ€»åˆ†
                    score['total_score'] = round(
                        score['importance'] * 0.35 +
                        score['authority'] * 0.25 +
                        score['spread'] * 0.20 +
                        score['innovation'] * 0.10 +
                        score['timeliness'] * 0.10,
                        2
                    )
                    self.news_items.append({
                        **item,
                        'score': score,
                        'collection_time': datetime.now().isoformat(),
                        'fetch_method': 'RSS'
                    })
        
        # ============== ç¬¬äºŒä¼˜å…ˆçº§ï¼šå®˜æ–¹API ==============
        # Hacker Newsç­‰æä¾›ç¨³å®šAPIçš„æº
        
        # æ‰©å±•çš„æœç´¢å…³é”®è¯ï¼ˆå‚è€ƒé¡¹ç›®1ï¼‰
        web_queries = [
            'artificial intelligence',
            'AI machine learning',
            'ChatGPT OpenAI',
            'Claude Anthropic',
            'GPT-4 LLM',
            'AI regulation policy',
            'AI technology breakthrough',
            'Gemini Google AI',
            'Llama Meta AI',
            'AI startup funding'
        ]
        
        # Twitteræœç´¢å…³é”®è¯ï¼ˆå‚è€ƒé¡¹ç›®1ï¼‰
        twitter_keywords = [
            'AI OR artificial intelligence OR ChatGPT OR Claude OR GPT OR LLM',
            'OpenAI OR Anthropic OR DeepMind',
            'AI breakthrough OR AI release OR AI launch'
        ]
        
        # å®˜æ–¹åšå®¢URLï¼ˆæ‰©å±• - å¢åŠ ä¸‰å¤§AIå·¨å¤´å®˜æ–¹åšå®¢ï¼‰
        websites = [
            # Anthropicå®˜æ–¹
            'https://www.anthropic.com/news',
            'https://www.anthropic.com/research',
            # OpenAIå®˜æ–¹
            'https://openai.com/blog',
            'https://openai.com/news',
            # Google AIå®˜æ–¹
            'https://blog.google/technology/ai/',
            'https://ai.googleblog.com/',
            'https://deepmind.google/blog/',
            # å…¶ä»–AIå·¨å¤´
            'https://ai.meta.com/blog/',
            'https://blogs.nvidia.com/blog/category/deep-learning/',
            'https://www.microsoft.com/en-us/ai/blog/'
        ]
        
        # å¤šæºå¹¶è¡Œæ”¶é›†ï¼ˆRSSå·²å•ç‹¬è·å–ï¼Œè¿™é‡Œè·å–å…¶ä»–æºï¼‰
        with ThreadPoolExecutor(max_workers=8) as executor:
            web_future = executor.submit(self.search_news_from_web, web_queries)
            website_future = executor.submit(self.extract_from_websites, websites)
            twitter_future = executor.submit(self.search_from_twitter, twitter_keywords)
            reddit_future = executor.submit(self.search_from_reddit)
            chinese_future = executor.submit(self.search_from_chinese_sources)
            # Hacker News APIï¼ˆç¨³å®šå¯é ï¼‰
            hn_future = executor.submit(self.search_from_hacker_news)
            meta_ai_future = executor.submit(self.search_from_meta_ai_blog)
            ms_research_future = executor.submit(self.search_from_microsoft_research_blog)
            
            web_results = web_future.result()
            website_results = website_future.result()
            twitter_results = twitter_future.result()
            reddit_results = reddit_future.result()
            chinese_results = chinese_future.result()
            # è·å–æ–°æ•°æ®æºç»“æœ
            hn_results = hn_future.result()
            meta_ai_results = meta_ai_future.result()
            ms_research_results = ms_research_future.result()
        
        # å¤„ç†æœç´¢ç»“æœï¼ˆè¿‡æ»¤æ— URLçš„æ–°é—»ï¼‰
        for item in web_results:
            if isinstance(item, dict) and 'title' in item and 'url' in item:
                if self.is_valid_url(item['url']):
                    score = self.calculate_score(item)
                    self.news_items.append({
                        **item,
                        'score': score,
                        'collection_time': datetime.now().isoformat()
                    })
        
        # å¤„ç†ç½‘ç«™æå–ç»“æœ
        for result in website_results:
            if isinstance(result, dict) and 'title' in result:
                score = self.calculate_score(result)
                self.news_items.append({
                    **result,
                    'score': score,
                    'collection_time': datetime.now().isoformat()
                })
        
        # å¤„ç†Twitterç»“æœï¼ˆå‚è€ƒé¡¹ç›®1ï¼‰
        for tweet in twitter_results:
            if isinstance(tweet, dict):
                # æ„å»ºæ¨æ–‡æ•°æ®
                tweet_text = tweet.get('text', '')[:200]
                author = tweet.get('author', {})
                username = author.get('username', 'unknown') if isinstance(author, dict) else 'unknown'
                tweet_id = tweet.get('id', '')
                
                score = self.calculate_score({
                    'title': tweet_text[:100],
                    'source': f"Twitter @{username}"
                })
                
                # æ·»åŠ ç¤¾äº¤æŒ‡æ ‡åˆ°è¯„åˆ†
                engagement = tweet.get('engagement', {})
                likes = engagement.get('likes', 0) if isinstance(engagement, dict) else 0
                if likes > 1000:
                    score['total_score'] = min(10, score['total_score'] + 1)
                
                self.news_items.append({
                    'title': tweet_text,
                    'source': f"Twitter @{username}",
                    'url': f"https://twitter.com/{username}/status/{tweet_id}",
                    'publish_time': tweet.get('posted', self.yesterday),
                    'engagement': engagement,
                    'score': score,
                    'collection_time': datetime.now().isoformat()
                })
        
        # å¤„ç†Redditç»“æœ
        for post in reddit_results:
            if isinstance(post, dict) and 'title' in post:
                score = self.calculate_score(post)
                
                # Redditå¸–å­è¯„åˆ†åŠ æˆï¼ˆåŸºäºç‚¹èµæ•°ï¼‰
                engagement = post.get('engagement', {})
                likes = engagement.get('likes', 0) if isinstance(engagement, dict) else 0
                if likes > 500:
                    score['total_score'] = min(10, score['total_score'] + 0.5)
                
                self.news_items.append({
                    **post,
                    'score': score,
                    'collection_time': datetime.now().isoformat()
                })
        
        # å¤„ç†ä¸­æ–‡æ–°é—»æºç»“æœ
        for item in chinese_results:
            if isinstance(item, dict) and 'title' in item:
                if self.is_valid_url(item.get('url', '')):
                    score = self.calculate_score(item)
                    # ä¸­æ–‡æºæƒå¨æ€§åŠ æˆ
                    if item.get('source') in ['æœºå™¨ä¹‹å¿ƒ', 'é‡å­ä½', 'æ–°æ™ºå…ƒ']:
                        score['authority'] = min(10, score.get('authority', 5) + 1)
                        score['total_score'] = min(10, score['total_score'] + 0.2)
                    self.news_items.append({
                        **item,
                        'score': score,
                        'collection_time': datetime.now().isoformat()
                    })
        
        # å¤„ç†Hacker Newsç»“æœï¼ˆæ–°å¢ï¼‰
        for item in hn_results:
            if isinstance(item, dict) and 'title' in item:
                if self.is_valid_url(item.get('url', '')):
                    score = self.calculate_score(item)
                    # HNé«˜åˆ†å¸–å­åŠ æˆï¼ˆY CombinatoræŠ•èµ„åŠ¨å‘çš„é£å‘æ ‡ï¼‰
                    engagement = item.get('engagement', {})
                    likes = engagement.get('likes', 0) if isinstance(engagement, dict) else 0
                    if likes > 500:
                        score['spread'] = min(10, score.get('spread', 5) + 2)
                        score['total_score'] = min(10, score['total_score'] + 0.5)
                    elif likes > 200:
                        score['spread'] = min(10, score.get('spread', 5) + 1)
                        score['total_score'] = min(10, score['total_score'] + 0.3)
                    self.news_items.append({
                        **item,
                        'score': score,
                        'collection_time': datetime.now().isoformat()
                    })
        
        # å¤„ç†Meta AI Blogç»“æœï¼ˆæ–°å¢ï¼‰
        for item in meta_ai_results:
            if isinstance(item, dict) and 'title' in item:
                if self.is_valid_url(item.get('url', '')):
                    score = self.calculate_score(item)
                    # Meta AIå®˜æ–¹åšå®¢æƒå¨æ€§åŠ æˆï¼ˆLlamaç³»åˆ—æ¨¡å‹å‘æºåœ°ï¼‰
                    score['authority'] = min(10, score.get('authority', 5) + 2)
                    score['total_score'] = min(10, score['total_score'] + 0.3)
                    self.news_items.append({
                        **item,
                        'score': score,
                        'collection_time': datetime.now().isoformat()
                    })
        
        # å¤„ç†Microsoft Research Blogç»“æœï¼ˆæ–°å¢ï¼‰
        for item in ms_research_results:
            if isinstance(item, dict) and 'title' in item:
                if self.is_valid_url(item.get('url', '')):
                    score = self.calculate_score(item)
                    # Microsoft Researchæƒå¨æ€§åŠ æˆï¼ˆPhiç³»åˆ—ã€AutoGenç­‰é‡è¦æˆæœï¼‰
                    score['authority'] = min(10, score.get('authority', 5) + 2)
                    score['total_score'] = min(10, score['total_score'] + 0.3)
                    self.news_items.append({
                        **item,
                        'score': score,
                        'collection_time': datetime.now().isoformat()
                    })
        
        self.log(f"æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(self.news_items)} æ¡æœ‰æ•ˆæ–°é—»")
        self.log(f"  ğŸ“¡ RSS Feeds: {len(rss_results)} æ¡ (ä¼˜å…ˆçº§æœ€é«˜)")
        self.log(f"  - ç½‘ç»œæœç´¢: {len(web_results)} æ¡")
        self.log(f"  - å®˜æ–¹åšå®¢: {len(website_results)} æ¡")
        self.log(f"  - Twitter: {len(twitter_results)} æ¡")
        self.log(f"  - Reddit: {len(reddit_results)} æ¡")
        self.log(f"  - ä¸­æ–‡æ–°é—»æº: {len(chinese_results)} æ¡")
        self.log(f"  - Hacker News API: {len(hn_results)} æ¡")
        self.log(f"  - Meta AI Blog: {len(meta_ai_results)} æ¡")
        self.log(f"  - Microsoft Research: {len(ms_research_results)} æ¡")
        
        return self.news_items
    
    def sort_and_filter(self, top_n=50):
        """æ’åºå¹¶ç­›é€‰æ–°é—»ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ›´æ™ºèƒ½çš„å»é‡ï¼‰"""
        if not self.news_items:
            return []
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        sorted_news = sorted(
            self.news_items, 
            key=lambda x: x.get('score', {}).get('total_score', 0), 
            reverse=True
        )
        
        # æ™ºèƒ½å»é‡
        unique_news = []
        seen_hashes = set()
        
        for news in sorted_news:
            # è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆåŸºäºæ ‡é¢˜å’ŒURLï¼‰
            content = f"{news.get('title', '')}{news.get('url', '')}"
            content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            
            # å¦‚æœå“ˆå¸Œé‡å¤ï¼Œè·³è¿‡
            if content_hash in seen_hashes:
                continue
            
            seen_hashes.add(content_hash)
            
            # åº”ç”¨ç¿»è¯‘
            translated_news = self.translate_news(news)
            unique_news.append(translated_news)
        
        return unique_news[:top_n]
    
    def save_reports(self):
        """ä¿å­˜æŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰"""
        sorted_news = self.sort_and_filter(50)
        date_str = datetime.now().strftime('%Y%m%d')
        
        # MarkdownæŠ¥å‘Š
        md_filename = f"ai_news_daily_{date_str}.md"
        md_filepath = os.path.join(self.output_dir, md_filename)
        
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(f"# AIæ–°é—»æ—¥æŠ¥ - {self.yesterday}\n\n")
            f.write(f"**é‡‡é›†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("**æ•°æ®æ¥æº**: ã€RSSä¼˜å…ˆã€‘TechCrunchã€Wiredã€MIT Technology Reviewã€Ars Technicaã€Hacker Newsã€arXivã€Hugging Faceã€The Gradient | Twitter/Xã€Redditã€OpenAIå®˜æ–¹åšå®¢ã€Anthropicå®˜æ–¹åšå®¢ã€Google AIå®˜æ–¹åšå®¢ã€Meta AI (FAIR) Blogã€Microsoft Research Blogã€DeepMindã€NVIDIAã€æœºå™¨ä¹‹å¿ƒã€é‡å­ä½ã€36æ°ª\n\n")
            f.write("---\n\n")
            
            for i, news in enumerate(sorted_news, 1):
                score = news.get('score', {})
                # æ˜¾ç¤ºä¸­æ–‡æ ‡é¢˜ï¼Œä½†ä¿ç•™åŸæ–‡æ ‡é¢˜
                display_title = news.get('title', 'æ— æ ‡é¢˜')
                original_title = news.get('original_title', '')
                if original_title and original_title != display_title:
                    display_title = f"{display_title}\nåŸæ–‡: {original_title}"
                
                f.write(f"### {i}. {display_title}\n\n")
                f.write(f"- **æ¥æº**: {news.get('source', 'æœªçŸ¥')}\n")
                f.write(f"- **å‘å¸ƒæ—¶é—´**: {news.get('publish_time', self.yesterday)}\n")
                f.write(f"- **ç»¼åˆè¯„åˆ†**: {score.get('total_score', 0)}/10\n")
                f.write(f"- **è¯„åˆ†æ˜ç»†**: é‡è¦æ€§{score.get('importance', 0)} | æƒå¨æ€§{score.get('authority', 0)} | ä¼ æ’­åº¦{score.get('spread', 0)} | åˆ›æ–°æ€§{score.get('innovation', 0)} | æ—¶æ•ˆæ€§{score.get('timeliness', 0)}\n")
                
                # ç¡®ä¿æœ‰åŸæ–‡é“¾æ¥
                url = news.get('url', '')
                if self.is_valid_url(url):
                    f.write(f"- **åŸæ–‡é“¾æ¥**: {url}\n")
                else:
                    f.write(f"- **åŸæ–‡é“¾æ¥**: æ— \n")
                
                content = news.get('content', '')
                if content:
                    f.write(f"- **å†…å®¹æ‘˜è¦**: {content}...\n\n")
                else:
                    f.write("\n")
                
                f.write("---\n\n")
        
        self.log(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_filepath}")
        
        # Top 10 JSONæ–‡ä»¶ï¼ˆä¾›æ¨é€è„šæœ¬ä½¿ç”¨ï¼‰
        top10 = sorted_news[:10]
        json_filename = f"ai_news_daily_{date_str}_top10.json"
        json_filepath = os.path.join(self.output_dir, json_filename)
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(top10, f, ensure_ascii=False, indent=2)
        
        self.log(f"Top 10 JSONå·²ä¿å­˜: {json_filepath}")
        
        return md_filepath, json_filepath, len(sorted_news)


def main():
    """ä¸»å‡½æ•°"""
    collector = AINewsCollector()
    
    try:
        # é‡‡é›†æ–°é—»
        collector.collect_all_news()
        
        # ä¿å­˜æŠ¥å‘Š
        md_path, json_path, total_count = collector.save_reports()
        
        collector.log(f"âœ… ä»»åŠ¡å®Œæˆï¼å…±å¤„ç† {total_count} æ¡æ–°é—»")
        collector.log(f"ğŸ“„ æŠ¥å‘Šè·¯å¾„: {md_path}")
        collector.log(f"ğŸ“Š JSONè·¯å¾„: {json_path}")
        
        sys.exit(0)
        
    except Exception as e:
        collector.log(f"âŒ ä»»åŠ¡å¤±è´¥: {e}", 'ERROR')
        import traceback
        collector.log(traceback.format_exc(), 'ERROR')
        sys.exit(1)


if __name__ == '__main__':
    main()
